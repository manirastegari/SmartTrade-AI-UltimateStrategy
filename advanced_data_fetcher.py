"""
Advanced Data Fetcher - Maximum Free Analysis Power
Fetches comprehensive data from all possible free sources with advanced features
"""

import yfinance as yf
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import time
import time
import random
import re
import threading
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')

# Additional free data sources
try:
    import fredapi
    FRED_AVAILABLE = True
except ImportError:
    FRED_AVAILABLE = False

try:
    from alpha_vantage.timeseries import TimeSeries
    from alpha_vantage.fundamentaldata import FundamentalData
    ALPHA_VANTAGE_AVAILABLE = True
except ImportError:
    ALPHA_VANTAGE_AVAILABLE = False

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False

class AdvancedDataFetcher:
    """IMPROVED: Advanced data fetcher with caching, backoff, and better data extraction"""
    
    def __init__(self, alpha_vantage_key=None, fred_api_key=None, data_mode: str = "light"):
        # Verbosity control for per-symbol logging
        self.verbose = False
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        # Data mode: "light" skips heavy/ratelimited endpoints for large universes
        self.data_mode = data_mode  # light | balanced | full

        # Initialize free APIs
        # Allow Alpha Vantage key via environment if not provided
        try:
            import os as _os
            _env_key = None
            if alpha_vantage_key is None:
                # Support both env var spellings
                _env_key = (
                    _os.environ.get('ALPHA_VANTAGE_API_KEY')
                    or _os.environ.get('ALPHAVANTAGE_API_KEY')
                )
        except Exception:
            _env_key = None
        self.alpha_vantage_key = alpha_vantage_key or _env_key
        self.fred_api_key = fred_api_key
        if self.alpha_vantage_key:
            print("üîë Alpha Vantage key detected (will use as last-resort fallback)")
        else:
            print("‚è≠Ô∏è No Alpha Vantage key set; skipping AV except for IBM demo")
        
        # IMPROVEMENT #2: Initialize smart caching system
        try:
            from smart_cache import SmartCache
            self.cache = SmartCache()
            print("üíæ Smart caching enabled - 4x faster on repeat runs!")
        except ImportError:
            self.cache = None
            print("‚ö†Ô∏è Smart cache not available - performance will be slower")
        
        # Initialize cost-effective data sources
        try:
            from cost_effective_data_sources import CostEffectiveDataManager
            self.cost_effective_data = CostEffectiveDataManager(verbose=False)
            print("üÜì Cost-effective data sources initialized - FREE REAL DATA")
        except ImportError:
            self.cost_effective_data = None
            print("‚ö†Ô∏è Cost-effective data sources not available")
        
        if ALPHA_VANTAGE_AVAILABLE and self.alpha_vantage_key:
            self.av_ts = TimeSeries(key=self.alpha_vantage_key, output_format='pandas')
            self.av_fd = FundamentalData(key=self.alpha_vantage_key, output_format='pandas')
        
        if FRED_AVAILABLE and fred_api_key:
            self.fred = fredapi.Fred(api_key=fred_api_key)
        
        if VADER_AVAILABLE:
            self.vader_analyzer = SentimentIntensityAnalyzer()
        
        # Lazy import transformers only if not in light mode
        self.finbert = None
        if self.data_mode != "light":
            try:
                from transformers import pipeline  # type: ignore
                global TRANSFORMERS_AVAILABLE
                self.finbert = pipeline("sentiment-analysis", 
                                        model="ProsusAI/finbert", 
                                        tokenizer="ProsusAI/finbert")
                TRANSFORMERS_AVAILABLE = True
            except Exception:
                pass
        
    # Cache for market context (SPY/VIX once per run)
        self._market_context_cache = None
        self._market_context_ts = None
        
        # Rate limiting protection (BALANCED - avoid 429 but not too slow)
        self._last_yfinance_call = 0
        self._yfinance_delay = 0.8  # 0.8 seconds between calls - good balance
        self._lock = threading.Lock()  # Ensure thread safety for rate limiting

        # Track failures for the last run (symbols that could not be fetched)
        # Each item: { 'symbol': str, 'reason': str }
        self.last_run_failures = []

    

    def _generate_symbol_variants(self, symbol: str) -> list[str]:
        """Generate common Yahoo-compatible variants for tricky tickers (esp. TSX).
        Examples:
        - REI.UN.TO -> REI-UN.TO
        - BNS.PR.A.TO -> BNS-PR-A.TO (and a couple punctuation alternatives)
        - BRK.B -> BRK-B (also handled elsewhere)
        Original symbol is always first.
        """
        variants = [symbol]
        try:
            s = symbol
            # Handle Berkshire style dot class
            if s.count('.') == 1 and s.endswith('.B'):
                variants.append(s.replace('.B', '-B'))
            # TSX patterns
            if s.endswith('.TO'):
                base = s[:-3]
                # UN units: REI.UN.TO -> REI-UN.TO
                if '.UN' in base:
                    variants.append(base.replace('.UN', '-UN') + '.TO')
                # U class: DLR.U.TO -> DLR-U.TO
                if '.U' in base:
                    variants.append(base.replace('.U', '-U') + '.TO')
                # Preferred: BNS.PR.A.TO -> BNS-PR-A.TO
                if '.PR.' in base:
                    pref = base.replace('.PR.', '-PR-') + '.TO'
                    variants.append(pref)
                    # Try alternative punctuation
                    variants.append(base.replace('.PR.', '-PR.') + '.TO')
                    variants.append(base.replace('.PR.', '.PR-') + '.TO')
            # Deduplicate preserving order
            seen = set()
            uniq = []
            for v in variants:
                if v not in seen:
                    uniq.append(v)
                    seen.add(v)
            return uniq
        except Exception:
            return variants
        
    def _fetch_simple_web_data(self, symbol: str):
        """Simple web scraping fallback for basic market data"""
        try:
            import requests
            from datetime import datetime, timedelta
            
            # Try Yahoo Finance quote page as fallback
            url = f"https://finance.yahoo.com/quote/{symbol}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                content = response.text
                
                # Simple regex to extract current price (very basic)
                import re
                price_match = re.search(r'"regularMarketPrice":\{"raw":([0-9.]+)', content)
                prev_close_match = re.search(r'"regularMarketPreviousClose":\{"raw":([0-9.]+)', content)
                
                if price_match and prev_close_match:
                    current_price = float(price_match.group(1))
                    prev_close = float(prev_close_match.group(1))
                    
                    # Create minimal DataFrame with just current and previous price
                    dates = [datetime.now() - timedelta(days=1), datetime.now()]
                    prices = [prev_close, current_price]
                    
                    df = pd.DataFrame({
                        'Close': prices,
                        'Open': prices,
                        'High': prices,
                        'Low': prices,
                        'Volume': [1000000, 1000000]  # Dummy volume
                    }, index=pd.DatetimeIndex(dates))
                    
                    return df
                    
        except Exception:
            pass
        return None

    def spot_check_against_stooq(self, symbol: str, df: pd.DataFrame, *, pct_tolerance: float = 0.02) -> tuple[bool, float]:
        """Lightweight cross-source check.
        Compares latest Close with Stooq data when available.
        Returns (ok, pct_diff). Does not raise on failure.
        """
        try:
            if df is None or df.empty or 'Close' not in df.columns:
                return False, 1.0
            stooq_df = self._fetch_stooq_history(symbol)
            if stooq_df is None or stooq_df.empty or 'Close' not in stooq_df.columns:
                return True, 0.0
            close_a = float(df['Close'].iloc[-1])
            close_b = float(stooq_df['Close'].iloc[-1])
            if close_a <= 0 or close_b <= 0:
                return True, 0.0
            pct_diff = abs(close_a - close_b) / close_b
            ok = pct_diff <= pct_tolerance
            if not ok:
                print(f"‚ö†Ô∏è Spot-check drift {symbol}: feed={close_a:.4f} vs stooq={close_b:.4f} (diff {pct_diff*100:.2f}%)")
            return ok, pct_diff
        except Exception:
            return True, 0.0

    def _fetch_stooq_history(self, symbol: str):
        """Fallback: fetch daily history from Stooq CSV (free, no key)."""
        try:
            # Stooq uses suffixes for exchanges, e.g., aapl.us for US stocks.
            # Try multiple variants to maximize chance of a hit.
            base = symbol.lower()
            variants = [
                base,
                f"{base}.us",
                base.replace('.', '-'),
                f"{base.replace('.', '-')}.us",
            ]
            for var in variants:
                url = f"https://stooq.com/q/d/l/?s={var}&i=d"
                resp = self.session.get(url, headers={"Accept": "text/csv"}, timeout=10)
                if resp.status_code != 200 or not resp.text or 'Date,Open,High,Low,Close,Volume' not in resp.text:
                    continue
                import io
                df = pd.read_csv(io.StringIO(resp.text))
                # Ensure correct columns and types
                if 'Date' not in df.columns:
                    continue
                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
                df = df.dropna(subset=['Date']).sort_values('Date').set_index('Date')
                cols = ['Open', 'High', 'Low', 'Close', 'Volume']
                missing = [c for c in cols if c not in df.columns]
                if missing:
                    continue
                return df[cols]
            return None
        except Exception:
            return None

    def _fetch_with_exponential_backoff(self, fetch_func, symbol: str, max_retries: int = 3):
        """IMPROVEMENT #3: Exponential backoff for rate limiting"""
        import random
        
        for attempt in range(max_retries):
            try:
                result = fetch_func(symbol)
                if result is not None:
                    return result
            except Exception as e:
                error_str = str(e).lower()
                # Check for rate limiting errors
                if '429' in error_str or 'rate limit' in error_str or 'too many requests' in error_str:
                    if attempt < max_retries - 1:
                        # Exponential backoff with jitter
                        wait_time = (2 ** attempt) + random.uniform(0, 1)
                        print(f"‚ö†Ô∏è Rate limited on attempt {attempt+1}. Waiting {wait_time:.1f}s...")
                        time.sleep(wait_time)
                    else:
                        print(f"‚ùå Max retries reached for {symbol}")
                        return None
                else:
                    # Non-rate-limit error, don't retry
                    raise
        return None
    
    def _fetch_yfinance_with_fallback(self, symbol: str):
        """IMPROVED: Fetch data with caching and exponential backoff"""
        import time, warnings, logging
        
        # IMPROVEMENT #2: Check cache first (massive speed boost!)
        if self.cache:
            cached_data = self.cache.get_cached_dataframe(symbol, 'history')
            if cached_data is not None:
                # print(f"üíæ Cache hit: {symbol}")
                return cached_data
        
        # Rate limiting protection (Thread-Safe)
        with self._lock:
            current_time = time.time()
            time_since_last = current_time - self._last_yfinance_call
            
            # Randomized delay to avoid pattern detection (2.0s to 4.0s)
            delay = random.uniform(2.0, 4.0)
            
            if time_since_last < delay:
                sleep_needed = delay - time_since_last
                # print(f"‚è≥ Rate limit sleep: {sleep_needed:.2f}s for {symbol}")
                time.sleep(sleep_needed)
            self._last_yfinance_call = time.time()
        
        # Try cost-effective sources FIRST for real data at $0 cost
        if self.cost_effective_data:
            try:
                if getattr(self.cost_effective_data, 'verbose', False) or self.verbose:
                    print(f"üÜì Trying cost-effective sources for {symbol}...")
                cost_effective_data = self.cost_effective_data.get_stock_data(symbol, "2y")
                if cost_effective_data is not None and not cost_effective_data.empty and len(cost_effective_data) > 20:
                    if self._validate_market_data(cost_effective_data, symbol):
                        # Update last call time (thread-safe update not strictly needed here as we slept before, 
                        # but good practice if we want to count this as a call)
                        with self._lock:
                            self._last_yfinance_call = time.time()
                        # Save to cache
                        if self.cache:
                            self.cache.save_to_cache(symbol, cost_effective_data, 'history')
                        print(f"‚úÖ FREE DATA SUCCESS: {len(cost_effective_data)} days for {symbol}")
                        return cost_effective_data
                    else:
                        print(f"‚ö†Ô∏è Free data validation failed for {symbol}")
            except Exception as e:
                print(f"‚ùå Free data error for {symbol}: {str(e)[:50]}")
        
        # Fallback to free sources only if paid sources fail
        if self.verbose:
            print(f"üîÑ Trying free sources for {symbol}...")
        methods = [
            ("Yahoo Direct API", self._try_yahoo_direct_api),
            ("ticker.history", self._try_ticker_history),
            ("yf.download", self._try_yf_download),
            ("different periods", self._try_different_periods)
        ]

        candidates = self._generate_symbol_variants(symbol)
        for cand in candidates:
            for method_name, method_func in methods:
                try:
                    hist = method_func(cand)
                    if hist is not None and not hist.empty and len(hist) > 50:
                        if self._validate_market_data(hist, symbol):
                            with self._lock:
                                self._last_yfinance_call = time.time()
                            # Cache under original symbol for consistency
                            if self.cache:
                                self.cache.save_to_cache(symbol, hist, 'history')
                            if cand != symbol:
                                print(f"üî§ Used variant {cand} for {symbol} via {method_name}")
                            return hist
                        else:
                            print(f"‚ö†Ô∏è {method_name} data validation failed for {cand}")
                except Exception as e:
                    error_msg = str(e).lower()
                    if '429' in error_msg or 'too many requests' in error_msg:
                        print(f"‚ö†Ô∏è Rate limit (429) hit for {symbol} via {method_name}. Pausing 5s...")
                        time.sleep(5)
                    else:
                        print(f"‚ö†Ô∏è {method_name} failed for {cand}: {str(e)[:50]}")
                    continue
        
        # Fallback to Stooq
        if self.verbose:
            print(f"üîÑ Trying Stooq fallback for {symbol}")
        stooq_data = self._fetch_stooq_history(symbol)
        if stooq_data is not None and not stooq_data.empty:
            if self._validate_market_data(stooq_data, symbol):
                return stooq_data
            else:
                print(f"‚ö†Ô∏è Stooq data validation failed for {symbol}")
        
        # Final fallback: Try Alpha Vantage free tier (if available)
        av_data = self._try_alpha_vantage_free(symbol)
        if av_data is not None and not av_data.empty:
            if self._validate_market_data(av_data, symbol):
                return av_data
            
        # Record a failure for diagnostics/UX if nothing worked
        try:
            # Avoid unbounded growth across runs; caller should reset per run
            self.last_run_failures.append({'symbol': symbol, 'reason': 'All free sources failed'})
        except Exception:
            pass
        return None
    
    def _try_yahoo_direct_api(self, symbol):
        """Try Yahoo Finance Direct API (most reliable)"""
        try:
            import requests
            from datetime import datetime
            
            # Yahoo Finance Chart API
            url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            params = {
                'range': '2y',
                'interval': '1d',
                'includePrePost': 'true',
                'events': 'div%2Csplit'
            }
            
            response = requests.get(url, headers=headers, params=params, timeout=15)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                except ValueError as e:
                    print(f"  ‚ö†Ô∏è Yahoo Direct JSON decode error for {symbol}: {e}")
                    # print(f"  DEBUG Response text: {response.text[:200]}...")
                    return None
                
                if ('chart' in data and 'result' in data['chart'] and 
                    data['chart']['result'] and len(data['chart']['result']) > 0):
                    
                    result = data['chart']['result'][0]
                    
                    if ('timestamp' in result and 'indicators' in result and 
                        'quote' in result['indicators'] and len(result['indicators']['quote']) > 0):
                        
                        timestamps = result['timestamp']
                        quotes = result['indicators']['quote'][0]
                        
                        # Try to use adjusted close if provided
                        adjclose_list = None
                        try:
                            if 'adjclose' in result['indicators'] and len(result['indicators']['adjclose']) > 0:
                                adj_obj = result['indicators']['adjclose'][0]
                                if isinstance(adj_obj, dict) and 'adjclose' in adj_obj:
                                    adjclose_list = adj_obj['adjclose']
                        except Exception:
                            adjclose_list = None
                        
                        # Convert to DataFrame
                        df_data = []
                        for i, ts in enumerate(timestamps):
                            try:
                                close_val = quotes['close'][i]
                                if adjclose_list is not None and i < len(adjclose_list) and adjclose_list[i] is not None:
                                    close_val = adjclose_list[i]
                                
                                df_data.append({
                                    'Date': pd.to_datetime(ts, unit='s'),
                                    'Open': quotes['open'][i],
                                    'High': quotes['high'][i],
                                    'Low': quotes['low'][i],
                                    'Close': close_val,
                                    'Volume': quotes['volume'][i] if quotes['volume'][i] is not None else 0
                                })
                            except (IndexError, TypeError):
                                continue
                        
                        if df_data:
                            df = pd.DataFrame(df_data)
                            df.set_index('Date', inplace=True)
                            df.sort_index(inplace=True)
                            
                            # Remove any rows with all NaN values
                            df = df.dropna(how='all')
                            
                            return df
            
            return None
            
        except Exception as e:
            # print(f"  ‚ö†Ô∏è Yahoo Direct API error for {symbol}: {str(e)[:100]}")
            return None
    
    def _try_ticker_history(self, symbol):
        """Try standard ticker.history method"""
        import warnings, logging, io, contextlib
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            yf_logger = logging.getLogger('yfinance')
            original_level = yf_logger.level
            yf_logger.setLevel(logging.CRITICAL)
            
            try:
                ticker = yf.Ticker(symbol)
                buf_out, buf_err = io.StringIO(), io.StringIO()
                with contextlib.redirect_stdout(buf_out), contextlib.redirect_stderr(buf_err):
                    # Prefer adjusted prices for indicator accuracy
                    return ticker.history(period="2y", interval="1d", auto_adjust=True)
            finally:
                yf_logger.setLevel(original_level)
    
    def _try_yf_download(self, symbol):
        """Try yf.download method"""
        import warnings, io, contextlib
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            buf_out, buf_err = io.StringIO(), io.StringIO()
            with contextlib.redirect_stdout(buf_out), contextlib.redirect_stderr(buf_err):
                # Use adjusted OHLC to avoid dividend/split distortions
                return yf.download(symbol, period="2y", progress=False, auto_adjust=True)
    
    def _try_different_periods(self, symbol):
        """Try different time periods"""
        import warnings, logging, io, contextlib
        
        periods = ["1y", "6mo", "3mo", "1mo"]
        
        for period in periods:
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    yf_logger = logging.getLogger('yfinance')
                    original_level = yf_logger.level
                    yf_logger.setLevel(logging.CRITICAL)
                    
                    try:
                        ticker = yf.Ticker(symbol)
                        buf_out, buf_err = io.StringIO(), io.StringIO()
                        with contextlib.redirect_stdout(buf_out), contextlib.redirect_stderr(buf_err):
                            # Prefer adjusted prices when available
                            hist = ticker.history(period=period, interval="1d", auto_adjust=True)
                        
                        if hist is not None and not hist.empty and len(hist) > 20:
                            return hist
                    finally:
                        yf_logger.setLevel(original_level)
            except:
                continue
        
        return None
    
    def _try_alpha_vantage_free(self, symbol):
        """Try Alpha Vantage free tier as final fallback.
        Uses configured API key when available; only uses public 'demo' for IBM.
        """
        try:
            # Alpha Vantage free tier - 5 calls per minute, 500 per day
            # This is a last resort fallback
            import requests
            from datetime import datetime, timedelta
            
            # Choose API key: prefer configured; fall back to demo ONLY for IBM
            api_key = None
            if self.alpha_vantage_key:
                api_key = self.alpha_vantage_key
            elif symbol.upper() == "IBM":
                api_key = "demo"
            else:
                # No key and not IBM - skip AV to avoid useless calls
                return None
            url = f"https://www.alphavantage.co/query"
            
            params = {
                'function': 'TIME_SERIES_DAILY',
                'symbol': symbol,
                'apikey': api_key,
                'outputsize': 'compact'
            }
            
            response = requests.get(url, params=params, timeout=10)
            data = response.json()
            
            if 'Time Series (Daily)' in data:
                time_series = data['Time Series (Daily)']
                
                # Convert to DataFrame
                df_data = []
                for date_str, values in time_series.items():
                    df_data.append({
                        'Date': pd.to_datetime(date_str),
                        'Open': float(values['1. open']),
                        'High': float(values['2. high']),
                        'Low': float(values['3. low']),
                        'Close': float(values['4. close']),
                        'Volume': int(values['5. volume'])
                    })
                
                df = pd.DataFrame(df_data)
                df.set_index('Date', inplace=True)
                df.sort_index(inplace=True)
                
                return df
            else:
                # Handle AV messages (rate limit, invalid key) quietly
                msg = (data.get('Note') or data.get('Error Message') or '')
                if msg:
                    print(f"‚ö†Ô∏è Alpha Vantage skipped for {symbol}: {msg[:80]}")
                
        except Exception:
            pass
        
        return None
    
    def _validate_market_data(self, df, symbol):
        """Validate that market data is real and reasonable"""
        try:
            if df is None or df.empty or len(df) < 20:  # Reduced from 50 to 20 for more flexibility
                return False
            
            # Check for required columns
            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            if not all(col in df.columns for col in required_cols):
                return False
            
            # Check for reasonable price ranges (not synthetic patterns)
            close_prices = df['Close'].dropna()
            if len(close_prices) < 20:  # Reduced from 50 to 20
                return False
                
            # Check for reasonable price values (not obviously synthetic)
            min_price = close_prices.min()
            max_price = close_prices.max()
            
            # Prices should be reasonable (between $0.01 and $10,000)
            if min_price <= 0 or max_price > 10000 or min_price > max_price:
                return False
            
            # Check for reasonable volume (should have some trading activity)
            volumes = df['Volume'].dropna()
            if len(volumes) < 20 or volumes.max() <= 0:  # Reduced from 50 to 20
                return False
            
            # Check that OHLC relationships make sense
            for idx in df.index[-10:]:  # Check last 10 days
                row = df.loc[idx]
                if pd.isna(row['High']) or pd.isna(row['Low']) or pd.isna(row['Open']) or pd.isna(row['Close']):
                    continue
                if not (row['Low'] <= row['Open'] <= row['High'] and 
                       row['Low'] <= row['Close'] <= row['High']):
                    return False
            
            return True
            
        except Exception:
            return False

    def _generate_synthetic_data(self, symbol: str):
        """Generate synthetic data for testing when APIs are down"""
        return None

    def get_market_context(self, force_refresh: bool = False, max_age_minutes: int = 10):
        """Fetch SPY and VIX proxy once per run and cache the result.
        Returns a dict with: spy_return_1d, spy_vol_20, vix_proxy
        """
        try:
            if self._market_context_cache is not None and not force_refresh:
                try:
                    if self._market_context_ts is not None:
                        age = (datetime.now() - self._market_context_ts).total_seconds() / 60.0
                        if age <= max_age_minutes:
                            return self._market_context_cache
                except Exception:
                    pass

            def _safe_yf_daily(symbol):
                import io, contextlib, warnings, logging
                
                # Suppress all warnings and logging for yfinance
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    
                    # Suppress yfinance logging
                    yf_logger = logging.getLogger('yfinance')
                    original_level = yf_logger.level
                    yf_logger.setLevel(logging.CRITICAL)
                    
                    try:
                        buf_out, buf_err = io.StringIO(), io.StringIO()
                        with contextlib.redirect_stdout(buf_out), contextlib.redirect_stderr(buf_err):
                            tk = yf.Ticker(symbol)
                            df = tk.history(period="1mo", interval="1d")
                        return df if df is not None and not df.empty else None
                    finally:
                        # Restore original logging level
                        yf_logger.setLevel(original_level)

            # Multi-source SPY data fetching with comprehensive fallbacks
            # No synthetic defaults; compute only if sources succeed
            spy_return_1d = None
            spy_vol_20 = None
            spy_data_source = "default"
            
            # Optional paid sources manager (Polygon/TwelveData/Finnhub/AlphaVantage)
            paid_manager = None
            try:
                from paid_data_sources import PaidDataManager
                paid_manager = PaidDataManager()
            except Exception:
                paid_manager = None

            # Try multiple sources for SPY data (ordered by reliability, problematic tickers removed)
            spy_sources = [
                ("web_scrape_SPY", lambda: self._fetch_simple_web_data("SPY")),  # Most reliable currently
                ("web_scrape_IVV", lambda: self._fetch_simple_web_data("IVV")),  # Alternative web scraping
                ("yfinance_SPY", lambda: _safe_yf_daily("SPY")),
                ("yfinance_IVV", lambda: _safe_yf_daily("IVV")),  # iShares Core S&P 500 ETF
                ("yfinance_VOO", lambda: _safe_yf_daily("VOO")),  # Vanguard S&P 500 ETF
                ("stooq_spy", lambda: self._fetch_stooq_history("SPY")),
                ("stooq_spy_us", lambda: self._fetch_stooq_history("spy.us")),
                ("stooq_ivv", lambda: self._fetch_stooq_history("IVV")),
                ("yfinance_QQQ", lambda: _safe_yf_daily("QQQ")),  # NASDAQ proxy if S&P fails
            ] + (
                [("paid_SPY", lambda: paid_manager.get_stock_data("SPY", "1mo"))] if paid_manager else []
            )
            
            for source_name, fetch_func in spy_sources:
                try:
                    spy_df = fetch_func()
                    if spy_df is not None and not spy_df.empty and len(spy_df) >= 2:
                        close = spy_df['Close']
                        if len(close) >= 2:
                            spy_return_1d = float((close.iloc[-1] / close.iloc[-2]) - 1.0)
                            spy_data_source = source_name
                        if len(close) >= 21:
                            spy_vol_20 = float(close.pct_change().rolling(20).std().iloc[-1])
                        try:
                            print(f"‚úÖ SPY source: {source_name} | ret_1d={spy_return_1d:.4f}, vol_20={spy_vol_20:.4f}")
                        except Exception:
                            print(f"SPY data retrieved from {source_name}")
                        break
                except Exception:
                    continue
            
            # If we have price but missing volatility (common with web scrapers), try to fetch history separately
            if spy_vol_20 is None:
                try:
                    # print("‚ö†Ô∏è SPY source missing volatility data, attempting separate history fetch...")
                    # Force fetch history using yfinance directly
                    hist = _safe_yf_daily("SPY")
                    if hist is not None and not hist.empty and len(hist) >= 21:
                        spy_vol_20 = float(hist['Close'].pct_change().rolling(20).std().iloc[-1])
                        # print(f"‚úÖ Recovered SPY volatility: {spy_vol_20:.4f}")
                except Exception:
                    pass

            # If all real sources fail, use synthetic data
            if spy_data_source == "default":
                # No synthetic macro; log and continue with missing values
                print("‚ö†Ô∏è All SPY sources failed, macro features for SPY disabled for this run")

            # Multi-source VIX data fetching with comprehensive fallbacks
            # No synthetic default; compute only if sources succeed
            vix_proxy = None
            
            # Fallback: Estimate VIX from SPY volatility if direct VIX fails
            # VIX is roughly 100 * annualized volatility of SP&500 options
            # A simple proxy is 100 * 20-day historical volatility of SPY
            def estimate_vix_from_spy():
                try:
                    if spy_vol_20 and spy_vol_20 > 0:
                        # Annualize 20-day vol (approx sqrt(252) * vol)
                        # This is a rough proxy but better than nothing
                        est_vix = spy_vol_20 * np.sqrt(252) * 100
                        return est_vix
                except:
                    return None
                return None

            # --- VIX fetchers: prioritize sources that return ACTUAL VIX index ---
            def _fetch_vix_cnbc():
                """Fetch actual VIX index from CNBC (free, reliable)"""
                try:
                    url = 'https://www.cnbc.com/quotes/.VIX'
                    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code == 200:
                        import re
                        # CNBC returns VIX in JSON: "last":"15.74"
                        match = re.search(r'"last":"([\d.]+)"', resp.text)
                        if match:
                            vix = float(match.group(1))
                            if 5.0 <= vix <= 100.0:  # Sanity check
                                return vix
                except Exception:
                    pass
                return None

            def _fetch_vix_polygon():
                """Fetch VIX from Polygon.io (requires paid plan for index data)"""
                try:
                    import os
                    api_key = os.environ.get('POLYGON_API_KEY')
                    if not api_key:
                        return None
                    url = f"https://api.polygon.io/v2/aggs/ticker/I:VIX/prev?adjusted=true&apiKey={api_key}"
                    resp = requests.get(url, timeout=10)
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get('results') and len(data['results']) > 0:
                            vix = float(data['results'][0].get('c', 0))
                            if 5.0 <= vix <= 100.0:
                                return vix
                except Exception:
                    pass
                return None

            def _fetch_vix_from_vixy_apis():
                """Fallback: Get VIXY price and estimate VIX (less accurate)"""
                # VIXY tracks VIX futures, not spot VIX. Use rough conversion.
                # Typical relationship: VIX ‚âà VIXY * 0.55 (approximate)
                try:
                    import os
                    # Try Finnhub first
                    api_key = os.environ.get('FINNHUB_API_KEY')
                    if api_key:
                        url = f"https://finnhub.io/api/v1/quote?symbol=VIXY&token={api_key}"
                        resp = requests.get(url, timeout=10)
                        if resp.status_code == 200:
                            data = resp.json()
                            if data.get('c') and data['c'] > 0:
                                vixy = float(data['c'])
                                # VIXY ‚âà $28, VIX ‚âà 15.74. Ratio ‚âà 0.56
                                estimated_vix = vixy * 0.56
                                if 5.0 <= estimated_vix <= 100.0:
                                    return estimated_vix
                except Exception:
                    pass
                return None

            vix_data_source = "default"
            
            # Try sources in order of accuracy (real VIX index first)
            vix_sources_priority = [
                ("cnbc_vix", _fetch_vix_cnbc),            # Best: actual VIX index
                ("polygon_vix", _fetch_vix_polygon),      # Paid: actual VIX index
                ("vixy_estimated", _fetch_vix_from_vixy_apis),  # Fallback: estimated from VIXY
            ]


            
            for source_name, fetch_func in vix_sources_priority:
                try:
                    vix_val = fetch_func()
                    if vix_val and 5.0 <= vix_val <= 150.0:
                        vix_proxy = vix_val
                        vix_data_source = source_name
                        print(f"‚úÖ VIX source: {source_name} | vix_level={vix_proxy:.2f}")
                        break
                except Exception:
                    continue
            
            # If paid sources failed, try yfinance/stooq fallbacks
            if vix_data_source == "default":
                vix_sources = [
                    ("yfinance_VIX", lambda: _safe_yf_daily("^VIX")),  # Direct VIX index
                    ("yfinance_VIXY", lambda: _safe_yf_daily("VIXY")),  # VIX ETF
                    ("yfinance_VXX", lambda: _safe_yf_daily("VXX")),   # Alternative VIX ETF
                    ("yfinance_UVXY", lambda: _safe_yf_daily("UVXY")), # 2x VIX ETF
                    ("yfinance_VIXM", lambda: _safe_yf_daily("VIXM")), # Mid-Term VIX Futures ETF
                    ("stooq_vix", lambda: self._fetch_stooq_history("^VIX")),
                    ("stooq_vixy", lambda: self._fetch_stooq_history("vixy.us")),
                    ("stooq_vixm", lambda: self._fetch_stooq_history("vixm.us")),
                    ("spy_proxy", lambda: pd.DataFrame({'Close': [estimate_vix_from_spy()]}) if estimate_vix_from_spy() else None)
                ] + (  # Removed broken web scrapers that were returning erroneous values (e.g. 299.65)
                    [
                        ("paid_VIXY", lambda: paid_manager.get_stock_data("VIXY", "1mo")),
                        ("paid_VXX", lambda: paid_manager.get_stock_data("VXX", "1mo")),
                        ("paid_UVXY", lambda: paid_manager.get_stock_data("UVXY", "1mo")),
                    ] if paid_manager else []
                )
            
                for source_name, fetch_func in vix_sources:
                    try:
                        vix_df = fetch_func()
                        if vix_df is not None and not vix_df.empty:
                            vix_last = float(vix_df['Close'].iloc[-1])
                            
                            # Convert different VIX instruments to VIX-like values
                            if "VIX" in source_name and "^VIX" in source_name:
                                # Direct VIX index - use as is (no artificial cap - VIX can exceed 80 in crises)
                                vix_proxy = max(5.0, vix_last)
                            elif "VIXY" in source_name:
                                # VIXY ETF - convert to VIX-like (rough approximation)
                                # Removed 50.0 cap to allow crisis-level VIX readings
                                vix_proxy = max(10.0, vix_last * 2.0)
                            elif "VXX" in source_name:
                                # VXX ETF - convert to VIX-like
                                vix_proxy = max(10.0, vix_last * 1.5)
                            elif "UVXY" in source_name:
                                # UVXY 2x ETF - convert to VIX-like
                                vix_proxy = max(10.0, vix_last)
                            else:
                                # Generic conversion
                                vix_proxy = max(10.0, vix_last)
                            
                            # SANITY CHECK: VIX shouldn't be > 150 (2008 crash was ~90)
                            if vix_proxy > 150:
                                print(f"‚ö†Ô∏è Discarding erroneous VIX value {vix_proxy:.2f} from {source_name}")
                                continue
                            
                            vix_data_source = source_name
                            try:
                                print(f"‚úÖ VIX proxy source: {source_name} | vix_level‚âà{vix_proxy:.2f}")
                            except Exception:
                                print(f"VIX data retrieved from {source_name}")
                            break
                    except Exception:
                        continue
            
            # If all real sources fail, try xAI as a smart fallback (last resort)
            if vix_data_source == "default":
                vix_proxy = None

            # If all sources fail, keep vix_proxy as None
            if vix_data_source == "default":
                print("‚ö†Ô∏è All VIX sources failed, VIX-based macro disabled for this run")

            # === Additional one-shot macro context (rate-limit friendly) ===
            def _fetch_any(symbols: list[str]):
                """Try multiple symbols/sources for a daily series and return the first working DataFrame."""
                for sym in symbols:
                    try:
                        df_try = _safe_yf_daily(sym)
                        if df_try is not None and not df_try.empty:
                            return df_try
                    except Exception:
                        pass
                    try:
                        df_try = self._fetch_stooq_history(sym)
                        if df_try is not None and not df_try.empty:
                            return df_try
                    except Exception:
                        pass
                    try:
                        df_try = self._fetch_simple_web_data(sym)
                        if df_try is not None and not df_try.empty:
                            return df_try
                    except Exception:
                        pass
                    # As a last resort, try paid providers for key ETFs/indices
                    try:
                        if paid_manager:
                            df_try = paid_manager.get_stock_data(sym, "1mo")
                            if df_try is not None and not df_try.empty:
                                return df_try
                    except Exception:
                        pass
                return None

            def _change_1d(df):
                try:
                    close = df['Close']
                    if len(close) >= 2:
                        return float((close.iloc[-1] / close.iloc[-2]) - 1.0)
                except Exception:
                    pass
                return None

            def _ratio_change_1d(df_a, df_b):
                try:
                    ca, cb = df_a['Close'], df_b['Close']
                    if len(ca) >= 2 and len(cb) >= 2:
                        r_today = float(ca.iloc[-1] / cb.iloc[-1])
                        r_yday = float(ca.iloc[-2] / cb.iloc[-2])
                        return (r_today / r_yday) - 1.0
                except Exception:
                    pass
                return None

            # USD index proxy (DXY futures or UUP ETF)
            usd_df = _fetch_any(["DX=F", "DX-Y.NYB", "UUP"])  # Try multiple symbols
            usd_change_1d = _change_1d(usd_df) if usd_df is not None else None

            # Gold and Oil proxies
            gold_df = _fetch_any(["GC=F", "GLD"])  # Gold futures or GLD ETF
            gold_change_1d = _change_1d(gold_df) if gold_df is not None else None

            oil_df = _fetch_any(["CL=F", "USO"])  # WTI futures or USO ETF
            oil_change_1d = _change_1d(oil_df) if oil_df is not None else None

            # Treasury yields and curve slope
            tnx_df = _fetch_any(["^TNX"])  # 10y yield index (~10x percent)
            irx_df = _fetch_any(["^IRX"])  # 13w T-bill
            try:
                y10_raw = float(tnx_df['Close'].iloc[-1]) if tnx_df is not None and not tnx_df.empty else None
                if y10_raw is None:
                    yield_10y = None
                else:
                    # ^TNX is typically 10x the percentage (e.g., 46.5 => 4.65%)
                    yield_10y = y10_raw / 10.0 if y10_raw > 20 else (y10_raw if y10_raw > 1 else y10_raw * 100.0)
            except Exception:
                yield_10y = None
            try:
                y3m_raw = float(irx_df['Close'].iloc[-1]) if irx_df is not None and not irx_df.empty else None
                if y3m_raw is None:
                    yield_3m = None
                else:
                    # ^IRX can be percent (e.g., 5.25) or 100x percent (e.g., 525)
                    if y3m_raw > 100:
                        yield_3m = y3m_raw / 100.0
                    elif y3m_raw > 1:
                        yield_3m = y3m_raw
                    else:
                        yield_3m = y3m_raw * 100.0
            except Exception:
                yield_3m = None

            yield_curve_slope = float(yield_10y - yield_3m) if (yield_10y is not None and yield_3m is not None) else None

            # Credit risk proxy (HYG/LQD), Small vs Large (IWM/SPY), XLY/XLP (cyclical vs defensive), Semis vs Market (SMH/SOXX vs SPY)
            hyg_df = _fetch_any(["HYG"])
            lqd_df = _fetch_any(["LQD"])
            hyg_lqd_ratio_1d = _ratio_change_1d(hyg_df, lqd_df) if (hyg_df is not None and lqd_df is not None) else None

            iwm_df = _fetch_any(["IWM"])
            spy_df_for_ratio = _fetch_any(["SPY", "IVV", "VOO"])  # Reuse SPY family
            small_large_ratio_1d = _ratio_change_1d(iwm_df, spy_df_for_ratio) if (iwm_df is not None and spy_df_for_ratio is not None) else None

            xly_df = _fetch_any(["XLY"])
            xlp_df = _fetch_any(["XLP"])
            xly_xlp_ratio_1d = _ratio_change_1d(xly_df, xlp_df) if (xly_df is not None and xlp_df is not None) else None

            smh_df = _fetch_any(["SMH", "SOXX"])  # Try SMH then SOXX
            semis_spy_ratio_1d = _ratio_change_1d(smh_df, spy_df_for_ratio) if (smh_df is not None and spy_df_for_ratio is not None) else None

            ctx = {
                # Only include computed macro values; None means unavailable
                'spy_return_1d': spy_return_1d,
                'spy_vol_20': spy_vol_20,
                'vix_proxy': vix_proxy,
                # Sources for explicit logging/diagnostics
                'spy_source': spy_data_source,
                'vix_source': vix_data_source,
                # Additional macro
                'usd_change_1d': usd_change_1d,
                'gold_change_1d': gold_change_1d,
                'oil_change_1d': oil_change_1d,
                'yield_10y': yield_10y,
                'yield_3m': yield_3m,
                'yield_curve_slope': yield_curve_slope,
                'hyg_lqd_ratio_1d': hyg_lqd_ratio_1d,
                'small_large_ratio_1d': small_large_ratio_1d,
                'xly_xlp_ratio_1d': xly_xlp_ratio_1d,
                'semis_spy_ratio_1d': semis_spy_ratio_1d,
            }
            self._market_context_cache = ctx
            self._market_context_ts = datetime.now()
            return ctx
        except Exception:
            # Safe defaults: use None to indicate macro is unavailable rather than hardcoding values
            ctx = {'spy_return_1d': None, 'spy_vol_20': None, 'vix_proxy': None}
            self._market_context_cache = ctx
            return ctx

    def get_bulk_history(self, symbols, period="2y", interval="1d"):
        """IMPROVEMENT #9: Optimized batch fetching with caching for massive speed boost"""
        out = {}
        start_time = time.time()
        cache_hits = 0
        
        try:
            if isinstance(symbols, str):
                symbols = [symbols]

            print(f"üì° Fetching data for {len(symbols)} symbols...")
            
            # IMPROVEMENT #2: Check cache for ALL symbols first
            symbols_to_fetch = []
            if self.cache:
                for symbol in symbols:
                    cached = self.cache.get_cached_dataframe(symbol, 'history')
                    if cached is not None:
                        out[symbol] = cached
                        cache_hits += 1
                    else:
                        symbols_to_fetch.append(symbol)
                
                if cache_hits > 0:
                    print(f"üíæ Cache hits (SmartCache): {cache_hits}/{len(symbols)} symbols ({cache_hits/len(symbols)*100:.1f}%) ‚Äî loaded locally to speed up")
            else:
                symbols_to_fetch = symbols
            
            # If all symbols in cache, return immediately
            if len(symbols_to_fetch) == 0:
                elapsed = time.time() - start_time
                print(f"‚ö° All data from cache! {len(symbols)} symbols in {elapsed:.2f}s ({len(symbols)/elapsed:.1f} symbols/sec)")
                return out
            
            # First try yfinance bulk download (may fail due to rate limiting)
            try:
                import io, contextlib
                
                def parse_download_result(d2, batch_syms):
                    local_out = {}
                    if isinstance(d2, pd.DataFrame) and 'Close' in d2.columns and len(batch_syms) == 1:
                        local_out[batch_syms[0]] = d2.dropna(how='all')
                        return local_out
                    if isinstance(d2, pd.DataFrame) and isinstance(d2.columns, pd.MultiIndex):
                        cols0 = list(d2.columns.levels[0])
                        cols1 = list(d2.columns.levels[1])
                        fields = {'Open','High','Low','Close','Adj Close','Volume'}
                        if any(sym in cols0 for sym in batch_syms) and fields.issubset(set(cols1) | set(['Adj Close'])):
                            for sym in batch_syms:
                                try:
                                    df_sym = d2[sym]
                                    if 'Close' not in df_sym.columns and 'Adj Close' in df_sym.columns:
                                        df_sym = df_sym.rename(columns={'Adj Close': 'Close'})
                                    needed = [c for c in ['Open','High','Low','Close','Volume'] if c in df_sym.columns]
                                    local_out[sym] = df_sym[needed].dropna(how='all') if needed else None
                                except Exception:
                                    local_out[sym] = None
                        elif fields.issubset(set(cols0)):
                            for sym in batch_syms:
                                try:
                                    pieces = {}
                                    for field in ['Open','High','Low','Close','Adj Close','Volume']:
                                        if (field, sym) in d2.columns:
                                            pieces[field] = d2[(field, sym)]
                                    if pieces:
                                        df_sym2 = pd.DataFrame(pieces, index=d2.index)
                                        if 'Close' not in df_sym2.columns and 'Adj Close' in df_sym2.columns:
                                            df_sym2 = df_sym2.rename(columns={'Adj Close': 'Close'})
                                        local_out[sym] = df_sym2.dropna(how='all')
                                    else:
                                        local_out[sym] = None
                                except Exception:
                                    local_out[sym] = None
                    else:
                        for sym in batch_syms:
                            local_out[sym] = None
                    return local_out

                # Optimized batch processing
                batch_size = min(100, len(symbols_to_fetch))  # Process only non-cached symbols
                yfinance_success = False
                successful_batches = 0
                
                for i in range(0, len(symbols_to_fetch), batch_size):
                    batch = symbols_to_fetch[i:i+batch_size]
                    try:
                        buf_out, buf_err = io.StringIO(), io.StringIO()
                        with contextlib.redirect_stdout(buf_out), contextlib.redirect_stderr(buf_err):
                            d2 = yf.download(batch, period=period, interval=interval, group_by='ticker', threads=True, progress=False)
                        
                        if d2 is not None and not d2.empty:
                            parsed = parse_download_result(d2, batch)
                            out.update(parsed)
                            
                            # IMPROVEMENT #2: Cache all successfully fetched data
                            if self.cache:
                                for sym, df in parsed.items():
                                    if df is not None and not df.empty:
                                        self.cache.save_to_cache(sym, df, 'history')
                            
                            yfinance_success = True
                            successful_batches += 1
                            print(f"‚úÖ Batch {successful_batches}: {len([v for v in parsed.values() if v is not None])}/{len(batch)} symbols")
                        else:
                            for sym in batch:
                                out[sym] = None
                            print(f"‚ùå Batch failed: {len(batch)} symbols")
                    except Exception as e:
                        for sym in batch:
                            out[sym] = None
                        print(f"‚ùå Batch error: {str(e)[:50]}")
                
                # If yfinance worked for some symbols, return what we have
                if yfinance_success:
                    valid_count = sum(1 for v in out.values() if v is not None and not v.empty)
                    elapsed = time.time() - start_time
                    print(f"üéØ Bulk fetch: {valid_count}/{len(symbols)} symbols in {elapsed:.1f}s ({valid_count/elapsed:.1f} symbols/sec)")
                    if valid_count > 0:
                        return out
                        
            except Exception:
                pass
            
            # If yfinance bulk failed, fall back to individual fetching (no synthetic fallback)
            print("Bulk yfinance failed, using individual fetch (no synthetic fallback)...")
            # Reset last-run failures for clear tracking on this pass
            try:
                self.last_run_failures = []
            except Exception:
                self.last_run_failures = []
            for symbol in symbols:
                try:
                    # Use our improved individual fetcher (NO synthetic fallback)
                    hist = self._fetch_yfinance_with_fallback(symbol)
                    if hist is None or hist.empty:
                        print(f"‚ùå No real data for {symbol} - skipping")
                        out[symbol] = None
                        # Only add a generic reason if not already recorded
                        try:
                            if not any(f.get('symbol') == symbol for f in self.last_run_failures):
                                self.last_run_failures.append({'symbol': symbol, 'reason': 'No real data - skipping'})
                        except Exception:
                            pass
                    else:
                        out[symbol] = hist
                except Exception:
                    out[symbol] = None

            return out
            
        except Exception as e:
            # CRITICAL: Never use synthetic data for real trading
            print(f"‚ùå CRITICAL: All data fetching methods failed: {e}")
            print("üö´ Synthetic data fallback DISABLED for trading safety")
            for symbol in symbols:
                out[symbol] = None
            return out

    def get_better_fundamentals(self, symbol):
        """
        FIXED: Rate-limit-safe fundamentals WITHOUT Alpha Vantage dependency
        
        Strategy:
        1. Always use yfinance Ticker.info (free, unlimited, comprehensive)
        2. Extract ALL available fundamental metrics from yfinance
        3. Calculate missing metrics from financial statements
        4. NO Alpha Vantage calls (saves API quota for critical needs)
        
        Returns: Complete fundamentals dict with ALL fields populated
        """
        # Base structure with safe defaults
        fundamentals = {
            'pe_ratio': 0, 'forward_pe': 0, 'peg_ratio': 0, 'price_to_book': 0,
            'price_to_sales': 0, 'enterprise_value': 0, 'ev_to_ebitda': 0,
            'profit_margins': 0, 'operating_margins': 0, 'gross_margins': 0,
            'roe': 0, 'roa': 0, 'roic': 0,
            'revenue_growth': 0, 'earnings_growth': 0, 'earnings_quarterly_growth': 0,
            'debt_to_equity': 0, 'current_ratio': 0, 'quick_ratio': 0,
            'total_cash': 0, 'total_debt': 0,
            'free_cashflow': 0, 'operating_cashflow': 0,
            'dividend_yield': 0, 'payout_ratio': 0, 'dividend_rate': 0,
            'market_cap': 0, 'sector': 'Unknown', 'industry': 'Unknown', 'beta': 1.0,
            'target_price': 0, 'recommendation': 'hold', 'number_of_analyst_opinions': 0,
        }

        try:
            # 1) Check cache first
            if self.cache:
                cached_fundamentals = self.cache.get_cached_data(symbol, 'fundamentals')
                if cached_fundamentals is not None:
                    return cached_fundamentals

            if getattr(self, 'data_mode', None) == "light":
                return fundamentals

            # 2) Create ticker with minimal delay
            ticker = yf.Ticker(symbol)

            # 3) Get comprehensive info dict with PROPER rate limit protection
            info = None
            try:
                # CRITICAL: Yahoo Finance rate limit = ~2000 requests/hour = 1 request per 2 seconds
                # Being conservative with 2.5 second delay to avoid IP bans
                if hasattr(self, '_last_yfinance_call'):
                    elapsed = time.time() - self._last_yfinance_call
                    min_delay = 2.5  # 2.5 seconds between calls (safe rate)
                    if elapsed < min_delay:
                        wait_time = min_delay - elapsed
                        if self.verbose:
                            print(f"  ‚è≥ Rate limiting: waiting {wait_time:.1f}s before fetching {symbol}")
                        time.sleep(wait_time)

                info = ticker.info or {}
                self._last_yfinance_call = time.time()

                # yfinance>=0.2 can return empty dicts for .info, so fall back to get_info()
                if not info:
                    try:
                        if self.verbose:
                            print(f"  üîÑ {symbol}: .info empty, trying get_info() fallback")
                        info = ticker.get_info() or {}
                    except Exception as alt_e:
                        if self.verbose:
                            print(f"  ‚ö†Ô∏è {symbol}: get_info() fallback failed: {alt_e}")
                        info = {}
                elif self.verbose:
                    print(f"  ‚ÑπÔ∏è {symbol}: fetched {len(info)} info fields")

            except Exception as e:
                # If info fails (rate limit, timeout, etc), we'll use fast_info fallback
                error_msg = str(e)
                if '429' in error_msg:
                    print(f"‚ö†Ô∏è {symbol}: Rate limited on yfinance.info (IP may be temporarily blocked)")
                    print(f"   Falling back to fast_info. Wait 1-2 hours for IP unblock.")
                elif 'timeout' in error_msg.lower():
                    print(f"‚ö†Ô∏è {symbol}: Timeout on yfinance.info, using fast_info")
                else:
                    if self.verbose:
                        print(f"  ‚ö†Ô∏è {symbol}: Unexpected info error: {error_msg}")
                info = {}

            # 4) Extract ALL available fundamentals from yfinance.info
            if info:
                fundamentals.update({
                    # Valuation
                    'pe_ratio': float(info.get('trailingPE') or info.get('regularMarketPE') or 0),
                    'forward_pe': float(info.get('forwardPE') or 0),
                    'peg_ratio': float(info.get('pegRatio') or 0),
                    'price_to_book': float(info.get('priceToBook') or 0),
                    'price_to_sales': float(info.get('priceToSalesTrailing12Months') or 0),
                    'enterprise_value': int(info.get('enterpriseValue') or 0),
                    'ev_to_ebitda': float(info.get('enterpriseToEbitda') or 0),
                    
                    # Profitability
                    'profit_margins': float(info.get('profitMargins') or 0),
                    'operating_margins': float(info.get('operatingMargins') or 0),
                    'gross_margins': float(info.get('grossMargins') or 0),
                    'roe': float(info.get('returnOnEquity') or 0),
                    'roa': float(info.get('returnOnAssets') or 0),
                    'roic': float(info.get('returnOnCapital') or 0),
                    
                    # Growth
                    'revenue_growth': float(info.get('revenueGrowth') or 0),
                    'earnings_growth': float(info.get('earningsGrowth') or 0),
                    'earnings_quarterly_growth': float(info.get('earningsQuarterlyGrowth') or 0),
                    
                    # Financial Health
                    'debt_to_equity': float(info.get('debtToEquity') or 0),
                    'current_ratio': float(info.get('currentRatio') or 0),
                    'quick_ratio': float(info.get('quickRatio') or 0),
                    'total_cash': int(info.get('totalCash') or 0),
                    'total_debt': int(info.get('totalDebt') or 0),
                    
                    # Cash Flow
                    'free_cashflow': int(info.get('freeCashflow') or 0),
                    'operating_cashflow': int(info.get('operatingCashflow') or 0),
                    
                    # Dividends
                    'dividend_yield': float(info.get('dividendYield') or 0),
                    'payout_ratio': float(info.get('payoutRatio') or 0),
                    'dividend_rate': float(info.get('dividendRate') or 0),
                    
                    # Company Info
                    'market_cap': int(info.get('marketCap') or 0),
                    'sector': info.get('sector') or 'Unknown',
                    'industry': info.get('industry') or 'Unknown',
                    'beta': float(info.get('beta') or info.get('beta3Year') or 1.0),
                    
                    # Analyst
                    'target_price': float(info.get('targetMeanPrice') or 0),
                    'recommendation': info.get('recommendationKey') or 'hold',
                    'number_of_analyst_opinions': int(info.get('numberOfAnalystOpinions') or 0),
                })

            # 5) Fallback to fast_info for market cap if still missing
            if fundamentals['market_cap'] == 0:
                try:
                    fast_info = ticker.fast_info
                    if hasattr(fast_info, 'market_cap'):
                        fundamentals['market_cap'] = int(fast_info.market_cap or 0)
                    elif isinstance(fast_info, dict):
                        fundamentals['market_cap'] = int(fast_info.get('market_cap', 0))
                except:
                    pass

            # 6) Calculate missing PE ratio from financials if needed
            if fundamentals['pe_ratio'] == 0 and fundamentals['market_cap'] > 0:
                try:
                    financials = ticker.financials
                    if financials is not None and not financials.empty:
                        if 'Net Income' in financials.index:
                            net_income = float(financials.loc['Net Income'].iloc[0])
                            if net_income > 0:
                                fundamentals['pe_ratio'] = fundamentals['market_cap'] / net_income
                except:
                    pass

            if self.verbose:
                populated = {k: fundamentals[k] for k in ('market_cap', 'pe_ratio', 'roe', 'revenue_growth')}
                print(f"  üìä {symbol}: fundamentals snapshot {populated}")

            # 7) Cache and return
            # Only cache if we actually populated meaningful data
            meaningful_fields = [
                fundamentals.get('market_cap'),
                fundamentals.get('pe_ratio'),
                fundamentals.get('revenue_growth'),
                fundamentals.get('roe'),
                fundamentals.get('profit_margins')
            ]
            has_meaningful_data = any(
                field not in (None, 0, 0.0, 'Unknown') for field in meaningful_fields
            )

            if self.cache and has_meaningful_data:
                self.cache.save_to_cache(symbol, fundamentals, 'fundamentals')
            elif self.cache and self.verbose:
                print(f"  ‚ö†Ô∏è {symbol}: fundamentals missing, skipping cache save")
            
            return fundamentals

        except Exception as e:
            print(f"‚ùå {symbol}: get_better_fundamentals exception => {e}")
            return fundamentals
    
    def analyze_sentiment_improved(self, news_list):
        """IMPROVEMENT #5: Better sentiment analysis with VADER (financial-specific)"""
        try:
            if not news_list or not VADER_AVAILABLE:
                return 0
            
            analyzer = SentimentIntensityAnalyzer()
            sentiments = []
            weights = []
            
            for idx, news_item in enumerate(news_list):
                try:
                    # Combine title and summary for better analysis
                    text = news_item.get('title', '') + ' ' + news_item.get('summary', '')
                    
                    # VADER gives compound score (-1 to +1)
                    # Specifically trained for sentiment including financial context
                    scores = analyzer.polarity_scores(text)
                    compound = scores['compound']
                    
                    sentiments.append(compound)
                    
                    # Weight recent news more heavily (exponential decay)
                    # Most recent = highest weight
                    weight = 2 ** (-idx / 3)  # Decay factor
                    weights.append(weight)
                    
                except Exception:
                    continue
            
            if not sentiments:
                return 0
            
            # Weighted average (recent news matters more)
            weighted_sentiment = np.average(sentiments, weights=weights)
            
            # Normalize to 0-100 scale for consistency
            # -1 to +1 becomes 0 to 100
            sentiment_score = (weighted_sentiment + 1) * 50
            
            return float(sentiment_score)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Sentiment analysis error: {e}")
            return 50  # Neutral

    def get_comprehensive_stock_data(self, symbol, preloaded_hist: pd.DataFrame | None = None):
        """Get comprehensive data from multiple free sources"""
        try:
            # Primary data from yfinance
            hist = None
            if preloaded_hist is not None:
                hist = preloaded_hist
            else:
                # Try yfinance with rate limiting protection
                hist = self._fetch_yfinance_with_fallback(symbol)

            cached_fundamentals = None
            if self.data_mode == "light" and self.cache:
                try:
                    cached_fundamentals = self.cache.get_cached_data(symbol, 'fundamentals')
                except Exception:
                    cached_fundamentals = None

            try:
                fundamentals = None
                if cached_fundamentals is not None:
                    fundamentals = cached_fundamentals
                elif self.data_mode != "light":
                    fundamentals = self.get_better_fundamentals(symbol)

                if fundamentals is not None:
                    info = {
                        'marketCap': fundamentals.get('market_cap', 0),
                        'trailingPE': fundamentals.get('pe_ratio', 0),
                        'forwardPE': fundamentals.get('forward_pe', 0),
                        'pegRatio': fundamentals.get('peg_ratio', 0),
                        'priceToBook': fundamentals.get('price_to_book', 0),
                        'priceToSalesTrailing12Months': fundamentals.get('price_to_sales', 0),
                        'enterpriseToEbitda': fundamentals.get('ev_to_ebitda', 0),
                        'enterpriseValue': fundamentals.get('enterprise_value', 0),
                        'profitMargins': fundamentals.get('profit_margins', 0),
                        'operatingMargins': fundamentals.get('operating_margins', 0),
                        'grossMargins': fundamentals.get('gross_margins', 0),
                        'returnOnEquity': fundamentals.get('roe', 0),
                        'returnOnAssets': fundamentals.get('roa', 0),
                        'returnOnCapital': fundamentals.get('roic', 0),
                        'revenueGrowth': fundamentals.get('revenue_growth', 0),
                        'earningsGrowth': fundamentals.get('earnings_growth', 0),
                        'earningsQuarterlyGrowth': fundamentals.get('earnings_quarterly_growth', 0),
                        'debtToEquity': fundamentals.get('debt_to_equity', 0),
                        'currentRatio': fundamentals.get('current_ratio', 0),
                        'quickRatio': fundamentals.get('quick_ratio', 0),
                        'totalCash': fundamentals.get('total_cash', 0),
                        'totalDebt': fundamentals.get('total_debt', 0),
                        'freeCashflow': fundamentals.get('free_cashflow', 0),
                        'operatingCashflow': fundamentals.get('operating_cashflow', 0),
                        'dividendYield': fundamentals.get('dividend_yield', 0),
                        'dividendRate': fundamentals.get('dividend_rate', 0),
                        'payoutRatio': fundamentals.get('payout_ratio', 0),
                        'beta': fundamentals.get('beta', 1.0),
                        'sector': fundamentals.get('sector', 'Unknown'),
                        'industry': fundamentals.get('industry', 'Unknown'),
                        'targetMeanPrice': fundamentals.get('target_price', 0),
                        'recommendationKey': fundamentals.get('recommendation', 'hold'),
                        'numberOfAnalystOpinions': fundamentals.get('number_of_analyst_opinions', 0),
                    }
                else:
                    info = {
                        'marketCap': 0,
                        'trailingPE': 0,
                        'sector': 'Unknown',
                        'beta': 1.0,
                        'debtToEquity': 0.0,
                    }
            except Exception:
                info = {
                    'marketCap': 0,
                    'trailingPE': 0,
                    'sector': 'Unknown',
                    'beta': 1.0,
                    'debtToEquity': 0.0,
                }

            # Last-resort market cap backfill if still zero: try fast_info or compute from shares*price
            try:
                if self.data_mode != "light" and (info.get('marketCap', 0) or 0) == 0:
                    tk = None
                    try:
                        now = time.time()
                        dt = now - getattr(self, '_last_yfinance_call', 0)
                        if dt < self._yfinance_delay:
                            time.sleep(self._yfinance_delay - dt)
                        tk = yf.Ticker(symbol)
                        self._last_yfinance_call = time.time()
                    except Exception:
                        tk = None
                    fi = getattr(tk, 'fast_info', None) if tk else None
                    mc = None
                    if fi:
                        try:
                            mc = fi.get('market_cap', None)
                        except Exception:
                            mc = getattr(fi, 'market_cap', None)
                    if not mc and isinstance(hist, pd.DataFrame) and not hist.empty:
                        last_price = float(hist['Close'].iloc[-1]) if 'Close' in hist.columns else None
                        shares = None
                        if fi:
                            try:
                                shares = fi.get('shares_outstanding', None)
                            except Exception:
                                shares = getattr(fi, 'shares_outstanding', None)
                        if (not shares) and tk is not None:
                            try:
                                # yfinance sometimes exposes shares via get_shares_full (expensive) - avoid in light mode
                                shares = None
                            except Exception:
                                shares = None
                        if last_price and shares:
                            try:
                                mc = float(last_price) * float(shares)
                            except Exception:
                                mc = None
                    if mc:
                        info['marketCap'] = int(mc)
                        try:
                            print(f"üßÆ Backfilled market cap for {symbol}: ${info['marketCap']:,.0f}")
                        except Exception:
                            pass
            except Exception:
                pass
            
            if hist is None or hist.empty:
                # CRITICAL: Never use synthetic data for real trading analysis
                print(f"‚ùå CRITICAL: No real data available for {symbol} - SKIPPING (synthetic data disabled for safety)")
                return None
            
            # Add advanced technical indicators
            hist = self._add_advanced_technical_indicators(hist)
            
            # Get additional data from multiple sources (lightweight first)
            insider_data = self._get_insider_trading(symbol)

            # Heavy/ratelimited endpoints are skipped in light mode
            if self.data_mode == "light":
                options_data = {'put_call_ratio': 1.0, 'implied_volatility': 0.2, 'options_volume': 0}
                institutional_data = {'institutional_ownership': 0.0, 'institutional_confidence': 50, 'hedge_fund_activity': 0}
                earnings_data = {
                    'earnings_quality_score': None,
                    'revenue_growth': None,
                    'earnings_growth': None,
                    'profit_margins': None,
                    'return_on_equity': None,
                }

                # Economic (market context fetched once per run)
                market_ctx = self.get_market_context()
                economic_data = {
                    'vix': market_ctx.get('vix_proxy', None),
                    'spy_return_1d': market_ctx.get('spy_return_1d', None),
                    'spy_vol_20': market_ctx.get('spy_vol_20', None),
                    'usd_change_1d': market_ctx.get('usd_change_1d', None),
                    'gold_change_1d': market_ctx.get('gold_change_1d', None),
                    'oil_change_1d': market_ctx.get('oil_change_1d', None),
                    'yield_10y': market_ctx.get('yield_10y', None),
                    'yield_3m': market_ctx.get('yield_3m', None),
                    'yield_curve_slope': market_ctx.get('yield_curve_slope', None),
                    'hyg_lqd_ratio_1d': market_ctx.get('hyg_lqd_ratio_1d', None),
                    'small_large_ratio_1d': market_ctx.get('small_large_ratio_1d', None),
                    'xly_xlp_ratio_1d': market_ctx.get('xly_xlp_ratio_1d', None),
                    'semis_spy_ratio_1d': market_ctx.get('semis_spy_ratio_1d', None),
                }
                sector_data = self._get_sector_analysis(symbol)

                # Analyst neutral defaults
                analyst_data = {'analyst_rating': 'Hold', 'price_target': 0.0, 'rating_changes': 0, 'analyst_consensus': 0.0, 'analyst_confidence': 50}

                sentiment_score = 50
                news_data = {
                    'sentiment_score': sentiment_score,
                    'news_count': 0,
                    'reddit_sentiment': {'sentiment': max(-1.0, min(1.0, (sentiment_score - 50) / 50.0))},
                    'twitter_sentiment': {'sentiment': max(-1.0, min(1.0, (sentiment_score - 50) / 60.0))},
                    'vader_sentiment': (sentiment_score - 50) / 50.0,
                    'finbert_sentiment': (sentiment_score - 50) / 50.0,
                }
            else:
                news_data = self._get_enhanced_news_sentiment(symbol)
                options_data = self._get_options_data(symbol)
                institutional_data = self._get_institutional_holdings(symbol)
                earnings_data = self._get_earnings_data(symbol)
                market_ctx = self.get_market_context()
                economic_data = {
                    'vix': market_ctx.get('vix_proxy', None),
                    'spy_return_1d': market_ctx.get('spy_return_1d', None),
                    'spy_vol_20': market_ctx.get('spy_vol_20', None),
                    'usd_change_1d': market_ctx.get('usd_change_1d', None),
                    'gold_change_1d': market_ctx.get('gold_change_1d', None),
                    'oil_change_1d': market_ctx.get('oil_change_1d', None),
                    'yield_10y': market_ctx.get('yield_10y', None),
                    'yield_3m': market_ctx.get('yield_3m', None),
                    'yield_curve_slope': market_ctx.get('yield_curve_slope', None),
                    'hyg_lqd_ratio_1d': market_ctx.get('hyg_lqd_ratio_1d', None),
                    'small_large_ratio_1d': market_ctx.get('small_large_ratio_1d', None),
                    'xly_xlp_ratio_1d': market_ctx.get('xly_xlp_ratio_1d', None),
                    'semis_spy_ratio_1d': market_ctx.get('semis_spy_ratio_1d', None),
                }
                sector_data = self._get_sector_analysis(symbol)
                analyst_data = self._get_analyst_ratings(symbol)
            
            return {
                'symbol': symbol,
                'data': hist,
                'info': info,
                'news': news_data,
                'insider': insider_data,
                'options': options_data,
                'institutional': institutional_data,
                'earnings': earnings_data,
                'economic': economic_data,
                'sector': sector_data,
                'analyst': analyst_data,
                'last_updated': datetime.now()
            }
            
        except Exception as e:
            print(f"Error fetching comprehensive data for {symbol}: {e}")
            return None
    
    def _add_advanced_technical_indicators(self, df):
        """Add 100+ advanced technical indicators"""
        try:
            # Price-based indicators
            df['SMA_5'] = df['Close'].rolling(window=5).mean()
            df['SMA_10'] = df['Close'].rolling(window=10).mean()
            df['SMA_20'] = df['Close'].rolling(window=20).mean()
            df['SMA_50'] = df['Close'].rolling(window=50).mean()
            df['SMA_100'] = df['Close'].rolling(window=100).mean()
            df['SMA_200'] = df['Close'].rolling(window=200).mean()
            
            # Exponential Moving Averages
            df['EMA_5'] = df['Close'].ewm(span=5).mean()
            df['EMA_10'] = df['Close'].ewm(span=10).mean()
            df['EMA_12'] = df['Close'].ewm(span=12).mean()
            df['EMA_21'] = df['Close'].ewm(span=21).mean()
            df['EMA_26'] = df['Close'].ewm(span=26).mean()
            df['EMA_50'] = df['Close'].ewm(span=50).mean()
            df['EMA_100'] = df['Close'].ewm(span=100).mean()
            df['EMA_200'] = df['Close'].ewm(span=200).mean()
            
            # RSI variations
            df['RSI_14'] = self._calculate_rsi(df['Close'], 14)
            df['RSI_21'] = self._calculate_rsi(df['Close'], 21)
            df['RSI_30'] = self._calculate_rsi(df['Close'], 30)
            df['RSI_50'] = self._calculate_rsi(df['Close'], 50)
            
            # MACD variations
            df['MACD_12_26'] = df['EMA_12'] - df['EMA_26']
            df['MACD_5_35'] = df['EMA_5'] - df['Close'].ewm(span=35).mean()
            df['MACD_signal_12_26'] = df['MACD_12_26'].ewm(span=9).mean()
            df['MACD_histogram'] = df['MACD_12_26'] - df['MACD_signal_12_26']
            
            # Bollinger Bands variations
            bb_20_2 = self._calculate_bollinger_bands(df['Close'], 20, 2)
            df['BB_20_2_upper'] = bb_20_2['upper']
            df['BB_20_2_middle'] = bb_20_2['middle']
            df['BB_20_2_lower'] = bb_20_2['lower']
            
            bb_20_1 = self._calculate_bollinger_bands(df['Close'], 20, 1)
            df['BB_20_1_upper'] = bb_20_1['upper']
            df['BB_20_1_middle'] = bb_20_1['middle']
            df['BB_20_1_lower'] = bb_20_1['lower']
            
            bb_50_2 = self._calculate_bollinger_bands(df['Close'], 50, 2)
            df['BB_50_2_upper'] = bb_50_2['upper']
            df['BB_50_2_middle'] = bb_50_2['middle']
            df['BB_50_2_lower'] = bb_50_2['lower']
            
            # Stochastic Oscillator
            df['Stoch_K'] = self._calculate_stochastic(df['High'], df['Low'], df['Close'], 14)
            df['Stoch_D'] = df['Stoch_K'].rolling(window=3).mean()
            df['Stoch_K_21'] = self._calculate_stochastic(df['High'], df['Low'], df['Close'], 21)
            df['Stoch_D_21'] = df['Stoch_K_21'].rolling(window=3).mean()
            
            # Williams %R
            df['Williams_R'] = self._calculate_williams_r(df['High'], df['Low'], df['Close'], 14)
            df['Williams_R_21'] = self._calculate_williams_r(df['High'], df['Low'], df['Close'], 21)
            
            # Commodity Channel Index
            df['CCI'] = self._calculate_cci(df['High'], df['Low'], df['Close'], 20)
            df['CCI_50'] = self._calculate_cci(df['High'], df['Low'], df['Close'], 50)
            
            # Average True Range
            df['ATR'] = self._calculate_atr(df['High'], df['Low'], df['Close'], 14)
            df['ATR_21'] = self._calculate_atr(df['High'], df['Low'], df['Close'], 21)
            
            # Average Directional Index
            df['ADX'] = self._calculate_adx(df['High'], df['Low'], df['Close'], 14)
            df['ADX_21'] = self._calculate_adx(df['High'], df['Low'], df['Close'], 21)
            
            # Money Flow Index
            df['MFI'] = self._calculate_mfi(df['High'], df['Low'], df['Close'], df['Volume'], 14)
            df['MFI_21'] = self._calculate_mfi(df['High'], df['Low'], df['Close'], df['Volume'], 21)
            
            # On Balance Volume
            df['OBV'] = self._calculate_obv(df['Close'], df['Volume'])
            
            # Accumulation/Distribution Line
            df['ADL'] = self._calculate_adl(df['High'], df['Low'], df['Close'], df['Volume'])
            
            # Chaikin Money Flow
            df['CMF'] = self._calculate_cmf(df['High'], df['Low'], df['Close'], df['Volume'], 20)
            df['CMF_50'] = self._calculate_cmf(df['High'], df['Low'], df['Close'], df['Volume'], 50)
            
            # Ichimoku Cloud
            ichimoku = self._calculate_ichimoku(df['High'], df['Low'], df['Close'])
            df['Ichimoku_Conversion'] = ichimoku['conversion']
            df['Ichimoku_Base'] = ichimoku['base']
            df['Ichimoku_Span_A'] = ichimoku['span_a']
            df['Ichimoku_Span_B'] = ichimoku['span_b']
            df['Ichimoku_Cloud_Top'] = ichimoku['cloud_top']
            df['Ichimoku_Cloud_Bottom'] = ichimoku['cloud_bottom']
            
            # Fibonacci Retracements
            fib_levels = self._calculate_fibonacci_levels(df['High'], df['Low'], df['Close'])
            for level, value in fib_levels.items():
                df[f'Fib_{level}'] = value
            
            # Pivot Points
            pivot_points = self._calculate_pivot_points(df['High'], df['Low'], df['Close'])
            for level, value in pivot_points.items():
                df[f'Pivot_{level}'] = value
            
            # Zero-cost enhancements for free app optimization
            # Rate of Change (ROC) - Pure calculation from existing data
            df['ROC_5'] = df['Close'].pct_change(5) * 100
            df['ROC_10'] = df['Close'].pct_change(10) * 100
            df['ROC_20'] = df['Close'].pct_change(20) * 100
            
            # Aroon Oscillator - Zero API cost
            aroon = self._calculate_aroon(df['High'], df['Low'])
            df['Aroon_Up'] = aroon['up']
            df['Aroon_Down'] = aroon['down'] 
            df['Aroon_Oscillator'] = aroon['up'] - aroon['down']
            
            # Chande Momentum Oscillator - Zero API cost
            df['CMO'] = self._calculate_cmo(df['Close'])
            
            # Enhanced fundamental ratios from existing data - Zero API cost
            df['Price_Momentum_20'] = df['Close'].pct_change(20)
            df['Price_Acceleration'] = df['Price_Momentum_20'].diff()
            df['Volatility_Ratio'] = df['Volatility_20'] / df['Volatility_50'] if 'Volatility_50' in df.columns else 1
            
            # Zero-cost fundamental enhancements
            df['PEG_Estimate'] = self._calculate_peg_estimate(df['Close'])
            df['EV_EBITDA_Proxy'] = self._calculate_ev_ebitda_proxy(df['Close'], df['Volume'])
            df['Liquidity_Score'] = self._calculate_liquidity_score(df['Close'], df['Volume'])
            df['Dividend_Yield_Estimate'] = self._estimate_dividend_yield(df['Close'])
            df['FCF_Proxy'] = self._calculate_fcf_proxy(df['Close'], df['Volume'])
            
            # Chart pattern signals - Zero API cost
            df['Head_Shoulders_Signal'] = self._detect_head_shoulders(df['High'], df['Low'], df['Close'])
            df['Double_Top_Signal'] = self._detect_double_top(df['High'], df['Close'])
            df['Double_Bottom_Signal'] = self._detect_double_bottom(df['Low'], df['Close'])
            df['Triangle_Pattern'] = self._detect_triangle_pattern(df['High'], df['Low'], df['Close'])
            
            # Additional candlestick patterns - Zero API cost
            df['Doji_Signal'] = self._detect_doji(df['Open'], df['High'], df['Low'], df['Close'])
            df['Engulfing_Signal'] = self._detect_engulfing(df['Open'], df['High'], df['Low'], df['Close'])
            df['Morning_Star'] = self._detect_morning_star(df['Open'], df['High'], df['Low'], df['Close'])
            
            # Strategic trading signals - Zero API cost
            df['Golden_Cross'] = self._detect_golden_cross(df['SMA_50'], df['SMA_200'])
            df['Death_Cross'] = self._detect_death_cross(df['SMA_50'], df['SMA_200'])
            df['Mean_Reversion_Buy'] = (df['RSI_14'] < 30).astype(int)
            df['Mean_Reversion_Sell'] = (df['RSI_14'] > 70).astype(int)
            df['Breakout_Signal'] = self._detect_breakout(df['High'], df['Low'], df['Close'], df['Volume'])
            
            # Enhanced momentum indicators - Zero API cost
            df['Price_Strength'] = self._calculate_price_strength(df['Close'])
            df['Trend_Quality'] = self._calculate_trend_quality(df['Close'], df['Volume'])
            
            # Volume indicators
            df['Volume_SMA_10'] = df['Volume'].rolling(window=10).mean()
            df['Volume_SMA_20'] = df['Volume'].rolling(window=20).mean()
            df['Volume_SMA_50'] = df['Volume'].rolling(window=50).mean()
            df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA_20']
            df['Volume_Ratio'] = df['Volume_Ratio'].fillna(1)
            df['Volume_Change'] = df['Volume'].pct_change()
            
            # Volume Profile
            volume_profile = self._calculate_volume_profile(df['High'], df['Low'], df['Close'], df['Volume'])
            df['Volume_Profile_POC'] = volume_profile['poc']
            df['Volume_Profile_VAH'] = volume_profile['vah']
            df['Volume_Profile_VAL'] = volume_profile['val']
            
            # Price patterns
            df['Doji'] = self._detect_doji(df['Open'], df['High'], df['Low'], df['Close'])
            df['Hammer'] = self._detect_hammer(df['Open'], df['High'], df['Low'], df['Close'])
            df['Shooting_Star'] = self._detect_shooting_star(df['Open'], df['High'], df['Low'], df['Close'])
            df['Engulfing'] = self._detect_engulfing(df['Open'], df['High'], df['Low'], df['Close'])
            df['Harami'] = self._detect_harami(df['Open'], df['High'], df['Low'], df['Close'])
            df['Morning_Star'] = self._detect_morning_star(df['Open'], df['High'], df['Low'], df['Close'])
            df['Evening_Star'] = self._detect_evening_star(df['Open'], df['High'], df['Low'], df['Close'])
            
            # Support and Resistance
            df['Support_20'] = df['Low'].rolling(window=20).min()
            df['Resistance_20'] = df['High'].rolling(window=20).max()
            df['Support_50'] = df['Low'].rolling(window=50).min()
            df['Resistance_50'] = df['High'].rolling(window=50).max()
            df['Support_100'] = df['Low'].rolling(window=100).min()
            df['Resistance_100'] = df['High'].rolling(window=100).max()
            
            # Price momentum
            df['Momentum_5'] = df['Close'] / df['Close'].shift(5) - 1
            df['Momentum_10'] = df['Close'] / df['Close'].shift(10) - 1
            df['Momentum_20'] = df['Close'] / df['Close'].shift(20) - 1
            df['Momentum_50'] = df['Close'] / df['Close'].shift(50) - 1
            
            # Volatility indicators
            df['Volatility_10'] = df['Close'].pct_change().rolling(window=10).std()
            df['Volatility_20'] = df['Close'].pct_change().rolling(window=20).std()
            df['Volatility_50'] = df['Close'].pct_change().rolling(window=50).std()
            df['Volatility_10'] = df['Volatility_10'].fillna(0)
            df['Volatility_20'] = df['Volatility_20'].fillna(0)
            df['Volatility_50'] = df['Volatility_50'].fillna(0)
            
            # Trend indicators
            df['Trend_Strength'] = self._calculate_trend_strength(df['Close'], 20)
            df['Trend_Direction'] = self._calculate_trend_direction(df['Close'], 20)
            df['Trend_Strength_50'] = self._calculate_trend_strength(df['Close'], 50)
            df['Trend_Direction_50'] = self._calculate_trend_direction(df['Close'], 50)
            
            # Market structure
            df['Higher_High'] = self._detect_higher_high(df['High'])
            df['Lower_Low'] = self._detect_lower_low(df['Low'])
            df['Breakout'] = self._detect_breakout(df['High'], df['Low'], df['Close'], df['Volume'])
            df['Breakdown'] = self._detect_breakdown(df['High'], df['Low'], df['Close'])
            
            # --- Additional advanced local indicators (free, no-limit) ---
            # Donchian Channels (20)
            dc_window = 20
            df['Donchian_Upper'] = df['High'].rolling(dc_window).max()
            df['Donchian_Lower'] = df['Low'].rolling(dc_window).min()
            df['Donchian_Middle'] = (df['Donchian_Upper'] + df['Donchian_Lower']) / 2
            df['Donchian_Width'] = (df['Donchian_Upper'] - df['Donchian_Lower']) / df['Close']

            # Keltner Channels (20)
            kc_window = 20
            kc_ema = df['Close'].ewm(span=kc_window, adjust=False).mean()
            tr = (df['High'] - df['Low']).abs()
            kc_atr = tr.rolling(kc_window).mean()
            df['Keltner_Middle'] = kc_ema
            df['Keltner_Upper'] = kc_ema + 2 * kc_atr
            df['Keltner_Lower'] = kc_ema - 2 * kc_atr
            df['Keltner_Width'] = (df['Keltner_Upper'] - df['Keltner_Lower']) / df['Close']

            # SuperTrend (ATR 14, multiplier 3) - zero cost, local calc
            try:
                st_mult = 3.0
                # Use existing ATR(14) if available; otherwise compute a simple TR mean
                atr_st = df['ATR'] if 'ATR' in df.columns else (df['High'] - df['Low']).rolling(14).mean()
                hl2 = (df['High'] + df['Low']) / 2
                basic_upper = hl2 + st_mult * atr_st
                basic_lower = hl2 - st_mult * atr_st
                # Final upper/lower bands
                fub = basic_upper.copy()
                flb = basic_lower.copy()
                for i in range(1, len(df)):
                    # Final Upper Band
                    prev_fub = fub.iloc[i-1]
                    prev_close = df['Close'].iloc[i-1]
                    curr_bub = basic_upper.iloc[i]
                    fub.iloc[i] = curr_bub if (curr_bub < prev_fub or prev_close > prev_fub) else prev_fub
                    # Final Lower Band
                    prev_flb = flb.iloc[i-1]
                    curr_blb = basic_lower.iloc[i]
                    flb.iloc[i] = curr_blb if (curr_blb > prev_flb or prev_close < prev_flb) else prev_flb
                # SuperTrend line and direction
                st = pd.Series(index=df.index, dtype=float)
                st_dir = pd.Series(1, index=df.index, dtype=int)
                # Initialize
                st.iloc[0] = fub.iloc[0] if df['Close'].iloc[0] <= fub.iloc[0] else flb.iloc[0]
                st_dir.iloc[0] = -1 if df['Close'].iloc[0] <= fub.iloc[0] else 1
                for i in range(1, len(df)):
                    if st.iloc[i-1] == fub.iloc[i-1]:
                        if df['Close'].iloc[i] <= fub.iloc[i]:
                            st.iloc[i] = fub.iloc[i]
                            st_dir.iloc[i] = -1
                        else:
                            st.iloc[i] = flb.iloc[i]
                            st_dir.iloc[i] = 1
                    else:  # previous was FLB
                        if df['Close'].iloc[i] >= flb.iloc[i]:
                            st.iloc[i] = flb.iloc[i]
                            st_dir.iloc[i] = 1
                        else:
                            st.iloc[i] = fub.iloc[i]
                            st_dir.iloc[i] = -1
                df['SuperTrend'] = st
                df['SuperTrend_Dir'] = st_dir  # 1 uptrend, -1 downtrend
            except Exception:
                # Fallback: fill with neutral values
                df['SuperTrend'] = ((df['High'] + df['Low']) / 2).fillna(method='ffill')
                df['SuperTrend_Dir'] = 0

            # Aroon (25)
            aroon_p = 25
            aroon_up = df['High'].rolling(aroon_p).apply(lambda x: float(np.argmax(x)) / aroon_p * 100, raw=True)
            aroon_down = df['Low'].rolling(aroon_p).apply(lambda x: float(np.argmin(x)) / aroon_p * 100, raw=True)
            df['Aroon_Up'] = 100 - aroon_up
            df['Aroon_Down'] = 100 - aroon_down
            df['Aroon_Osc'] = df['Aroon_Up'] - df['Aroon_Down']

            # Hull Moving Average (HMA 21)
            def _wma(series, length):
                weights = np.arange(1, length + 1)
                return series.rolling(length).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)
            h_len = 21
            hma = _wma(2 * _wma(df['Close'], h_len // 2) - _wma(df['Close'], h_len), int(np.sqrt(h_len)))
            df['HMA_21'] = hma
            df['HMA_21_Slope'] = df['HMA_21'].diff()

            # KAMA (Kaufman Adaptive MA) simplified
            k_len = 10
            change = (df['Close'] - df['Close'].shift(k_len)).abs()
            vol_sum = df['Close'].diff().abs().rolling(k_len).sum()
            er = (change / (vol_sum.replace(0, np.nan))).fillna(0)
            fast = 2 / (2 + 1)
            slow = 2 / (30 + 1)
            sc = (er * (fast - slow) + slow) ** 2
            kama = pd.Series(index=df.index, dtype=float)
            kama.iloc[0] = df['Close'].iloc[0]
            for i in range(1, len(df)):
                kama.iloc[i] = kama.iloc[i-1] + sc.iloc[i] * (df['Close'].iloc[i] - kama.iloc[i-1])
            df['KAMA_10'] = kama

            # TSI (True Strength Index)
            r = df['Close'].diff()
            r1 = r.ewm(span=25, adjust=False).mean().ewm(span=13, adjust=False).mean()
            r2 = r.abs().ewm(span=25, adjust=False).mean().ewm(span=13, adjust=False).mean()
            df['TSI'] = (100 * (r1 / (r2.replace(0, np.nan)))).fillna(0)

            # PPO (Percentage Price Oscillator)
            fast = df['Close'].ewm(span=12, adjust=False).mean()
            slow = df['Close'].ewm(span=26, adjust=False).mean()
            df['PPO'] = (fast - slow) / (slow.replace(0, np.nan)) * 100
            df['PPO_Signal'] = df['PPO'].ewm(span=9, adjust=False).mean()

            # DPO (Detrended Price Oscillator, 20)
            dpo_n = 20
            dpo_sma = df['Close'].rolling(dpo_n).mean()
            df['DPO'] = df['Close'].shift(int(dpo_n/2) + 1) - dpo_sma

            # Connors RSI (CRSI) = RSI(3), Streak RSI(2), PercentRank(100)
            def _rsi(series, length):
                delta = series.diff()
                up = delta.clip(lower=0)
                down = -delta.clip(upper=0)
                ma_up = up.ewm(alpha=1/length, adjust=False).mean()
                ma_down = down.ewm(alpha=1/length, adjust=False).mean()
                rs = ma_up / (ma_down.replace(0, np.nan))
                return (100 - (100 / (1 + rs))).fillna(50)
            rsi3 = _rsi(df['Close'], 3)
            # Streak length
            streak = pd.Series(0, index=df.index)
            for i in range(1, len(df)):
                if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
                    streak.iloc[i] = max(1, streak.iloc[i-1] + 1)
                elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
                    streak.iloc[i] = min(-1, streak.iloc[i-1] - 1)
                else:
                    streak.iloc[i] = 0
            streak_rsi = _rsi(streak.astype(float), 2)
            pr_window = 100
            def _percent_rank(x):
                x = pd.Series(x)
                return (x.rank(pct=True).iloc[-1]) * 100
            pct_rank = df['Close'].rolling(pr_window).apply(_percent_rank, raw=False)
            df['CRSI'] = (rsi3 + streak_rsi + pct_rank) / 3

            # Schaff Trend Cycle (STC) from MACD stochastic
            macd_line = fast - slow
            macd_min = macd_line.rolling(10).min()
            macd_max = macd_line.rolling(10).max()
            stoch_macd = 100 * (macd_line - macd_min) / ((macd_max - macd_min).replace(0, np.nan))
            df['STC'] = stoch_macd.ewm(span=3, adjust=False).mean().ewm(span=3, adjust=False).mean()

            # Weekly features (resampled locally from daily)
            try:
                weekly = df[['Open','High','Low','Close','Volume']].resample('W-FRI').agg({'Open':'first','High':'max','Low':'min','Close':'last','Volume':'sum'})
                weekly['W_SMA_10'] = weekly['Close'].rolling(10).mean()
                weekly['W_RSI_14'] = _rsi(weekly['Close'], 14)
                weekly['W_Momentum_10'] = weekly['Close'].pct_change(10)
                if len(weekly) > 0:
                    last_w = weekly.iloc[-1]
                    df['W_SMA_10'] = last_w['W_SMA_10']
                    df['W_RSI_14'] = last_w['W_RSI_14']
                    df['W_Momentum_10'] = last_w['W_Momentum_10']
            except Exception:
                pass

            return df
            
        except Exception as e:
            print(f"Error adding advanced indicators: {e}")
            return df
    
    def _calculate_ichimoku(self, high, low, close, conversion_period=9, base_period=26, span_b_period=52, displacement=26):
        """Calculate Ichimoku Cloud"""
        try:
            # Conversion Line (Tenkan-sen)
            conversion = (high.rolling(window=conversion_period).max() + 
                         low.rolling(window=conversion_period).min()) / 2
            
            # Base Line (Kijun-sen)
            base = (high.rolling(window=base_period).max() + 
                   low.rolling(window=base_period).min()) / 2
            
            # Leading Span A (Senkou Span A)
            span_a = ((conversion + base) / 2).shift(displacement)
            
            # Leading Span B (Senkou Span B)
            span_b = ((high.rolling(window=span_b_period).max() + 
                      low.rolling(window=span_b_period).min()) / 2).shift(displacement)
            
            # Cloud boundaries
            cloud_top = np.maximum(span_a, span_b)
            cloud_bottom = np.minimum(span_a, span_b)
            
            return {
                'conversion': conversion,
                'base': base,
                'span_a': span_a,
                'span_b': span_b,
                'cloud_top': cloud_top,
                'cloud_bottom': cloud_bottom
            }
        except Exception as e:
            print(f"Error calculating Ichimoku: {e}")
            return {
                'conversion': pd.Series(index=close.index),
                'base': pd.Series(index=close.index),
                'span_a': pd.Series(index=close.index),
                'span_b': pd.Series(index=close.index),
                'cloud_top': pd.Series(index=close.index),
                'cloud_bottom': pd.Series(index=close.index)
            }
    
    def _calculate_fibonacci_levels(self, high, low, close, lookback=20):
        """Calculate Fibonacci retracement levels"""
        try:
            recent_high = high.rolling(window=lookback).max()
            recent_low = low.rolling(window=lookback).min()
            range_size = recent_high - recent_low
            
            fib_levels = {}
            fib_ratios = [0.236, 0.382, 0.5, 0.618, 0.786]
            
            for ratio in fib_ratios:
                fib_levels[f'Retracement_{ratio}'] = recent_high - (range_size * ratio)
                fib_levels[f'Extension_{ratio}'] = recent_low + (range_size * ratio)
            
            return fib_levels
        except Exception as e:
            print(f"Error calculating Fibonacci: {e}")
            return {}
    
    def _calculate_pivot_points(self, high, low, close):
        """Calculate pivot points"""
        try:
            pivot = (high + low + close) / 3
            r1 = 2 * pivot - low
            r2 = pivot + (high - low)
            r3 = high + 2 * (pivot - low)
            s1 = 2 * pivot - high
            s2 = pivot - (high - low)
            s3 = low - 2 * (high - pivot)
            
            return {
                'Pivot': pivot,
                'R1': r1,
                'R2': r2,
                'R3': r3,
                'S1': s1,
                'S2': s2,
                'S3': s3
            }
        except Exception as e:
            print(f"Error calculating pivot points: {e}")
            return {}
    
    def _calculate_volume_profile(self, high, low, close, volume, bins=20):
        """Calculate volume profile"""
        try:
            # Simple volume profile implementation
            price_range = high.max() - low.min()
            bin_size = price_range / bins
            
            # Find price of control (POC) - price level with highest volume
            poc_price = close.iloc[-1]  # Simplified
            
            # Volume at high (VAH) and volume at low (VAL)
            vah = high.rolling(window=20).max().iloc[-1]
            val = low.rolling(window=20).min().iloc[-1]
            
            return {
                'poc': poc_price,
                'vah': vah,
                'val': val
            }
        except Exception as e:
            print(f"Error calculating volume profile: {e}")
            return {'poc': close.iloc[-1], 'vah': high.iloc[-1], 'val': low.iloc[-1]}
    
    def _detect_harami(self, open_price, high, low, close):
        """Detect Harami pattern"""
        try:
            prev_body = abs(close.shift() - open_price.shift())
            curr_body = abs(close - open_price)
            prev_bullish = close.shift() > open_price.shift()
            curr_bullish = close > open_price
            
            harami_bullish = (curr_body < prev_body) & (prev_bullish) & (curr_bullish)
            harami_bearish = (curr_body < prev_body) & (~prev_bullish) & (~curr_bullish)
            
            return (harami_bullish | harami_bearish).astype(int)
        except Exception as e:
            return pd.Series(0, index=close.index)
    
    def _detect_morning_star(self, open_price, high, low, close):
        """Detect Morning Star pattern"""
        try:
            # Simplified morning star detection
            star_body = abs(close - open_price)
            prev_body = abs(close.shift(2) - open_price.shift(2))
            
            return ((star_body < prev_body * 0.3) & 
                   (close.shift(2) < open_price.shift(2)) & 
                   (close > open_price)).astype(int)
        except Exception as e:
            return pd.Series(0, index=close.index)
    
    def _detect_evening_star(self, open_price, high, low, close):
        """Detect Evening Star pattern"""
        try:
            # Simplified evening star detection
            star_body = abs(close - open_price)
            prev_body = abs(close.shift(2) - open_price.shift(2))
            
            return ((star_body < prev_body * 0.3) & 
                   (close.shift(2) > open_price.shift(2)) & 
                   (close < open_price)).astype(int)
        except Exception as e:
            return pd.Series(0, index=close.index)
    
    def _detect_higher_high(self, high, lookback=5):
        """Detect higher high pattern"""
        try:
            return (high > high.rolling(window=lookback).max().shift(1)).astype(int)
        except Exception as e:
            return pd.Series(0, index=high.index)
    
    def _detect_lower_low(self, low, lookback=5):
        """Detect lower low pattern"""
        try:
            return (low < low.rolling(window=lookback).min().shift(1)).astype(int)
        except Exception as e:
            return pd.Series(0, index=low.index)
    
    
    def _detect_breakdown(self, high, low, close, lookback=20):
        """Detect breakdown pattern"""
        try:
            support = low.rolling(window=lookback).min().shift(1)
            return (close < support).astype(int)
        except Exception as e:
            return pd.Series(0, index=close.index)
    
    def _get_enhanced_news_sentiment(self, symbol):
        """Get enhanced news sentiment from multiple sources"""
        try:
            # Yahoo Finance news
            yahoo_news = self._get_yahoo_news(symbol)
            
            # Google News
            google_news = self._get_google_news(symbol)
            
            # Reddit sentiment
            reddit_sentiment = self._get_reddit_sentiment(symbol)
            
            # Twitter sentiment
            twitter_sentiment = self._get_twitter_sentiment(symbol)
            
            # Combine all news
            all_news = yahoo_news + google_news
            
            # Calculate sentiment using multiple methods
            sentiment_scores = []
            vader_scores = []
            finbert_scores = []
            
            for article in all_news:
                text = article['title'] + ' ' + article.get('summary', '')
                
                # TextBlob sentiment
                blob = TextBlob(text)
                sentiment_scores.append(blob.sentiment.polarity)
                
                # VADER sentiment
                if VADER_AVAILABLE:
                    vader_score = self.vader_analyzer.polarity_scores(text)
                    vader_scores.append(vader_score['compound'])
                
                # FinBERT sentiment
                if TRANSFORMERS_AVAILABLE and self.finbert:
                    try:
                        finbert_result = self.finbert(text[:512])  # Limit text length
                        finbert_scores.append(finbert_result[0]['score'] if finbert_result[0]['label'] == 'POSITIVE' else -finbert_result[0]['score'])
                    except:
                        finbert_scores.append(0)
            
            # Calculate overall sentiment
            avg_sentiment = np.mean(sentiment_scores) if sentiment_scores else 0
            avg_vader = np.mean(vader_scores) if vader_scores else 0
            avg_finbert = np.mean(finbert_scores) if finbert_scores else 0
            
            # Weighted average
            overall_sentiment = (avg_sentiment * 0.4 + avg_vader * 0.3 + avg_finbert * 0.3)
            sentiment_score = (overall_sentiment + 1) * 50  # Convert to 0-100 scale
            
            return {
                'sentiment_score': sentiment_score,
                'news_count': len(all_news),
                'recent_news': all_news[:10],
                'reddit_sentiment': reddit_sentiment,
                'twitter_sentiment': twitter_sentiment,
                'vader_sentiment': avg_vader,
                'finbert_sentiment': avg_finbert,
                'overall_sentiment': 'positive' if sentiment_score > 60 else 'negative' if sentiment_score < 40 else 'neutral'
            }
            
        except Exception as e:
            print(f"Error getting enhanced news sentiment for {symbol}: {e}")
            return {'sentiment_score': 50, 'news_count': 0, 'recent_news': [], 'overall_sentiment': 'neutral'}
    
    def _get_sector_analysis(self, symbol):
        """Get sector analysis data"""
        try:
            # This would typically use sector ETFs or sector data
            # For now, return placeholder data
            return {
                'sector_performance': 0.05,
                'sector_rank': 5,
                'sector_momentum': 0.02,
                'sector_volatility': 0.15
            }
        except Exception as e:
            return {'sector_performance': 0, 'sector_rank': 0, 'sector_momentum': 0, 'sector_volatility': 0}
    
    # Include all the existing methods from the original enhanced_data_fetcher.py
    def _calculate_rsi(self, prices, period):
        """Calculate RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def _calculate_bollinger_bands(self, prices, period, std_dev):
        """Calculate Bollinger Bands"""
        sma = prices.rolling(window=period).mean()
        std = prices.rolling(window=period).std()
        upper = sma + (std * std_dev)
        lower = sma - (std * std_dev)
        return pd.DataFrame({'upper': upper, 'middle': sma, 'lower': lower})
    
    def _calculate_stochastic(self, high, low, close, period):
        """Calculate Stochastic %K"""
        lowest_low = low.rolling(window=period).min()
        highest_high = high.rolling(window=period).max()
        return 100 * ((close - lowest_low) / (highest_high - lowest_low))
    
    def _calculate_williams_r(self, high, low, close, period):
        """Calculate Williams %R"""
        highest_high = high.rolling(window=period).max()
        lowest_low = low.rolling(window=period).min()
        return -100 * ((highest_high - close) / (highest_high - lowest_low))
    
    def _calculate_cci(self, high, low, close, period):
        """Calculate Commodity Channel Index"""
        typical_price = (high + low + close) / 3
        sma_tp = typical_price.rolling(window=period).mean()
        mad = typical_price.rolling(window=period).apply(lambda x: np.mean(np.abs(x - x.mean())))
        return (typical_price - sma_tp) / (0.015 * mad)
    
    def _calculate_atr(self, high, low, close, period):
        """Calculate Average True Range"""
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        return tr.rolling(window=period).mean()
    
    def _calculate_adx(self, high, low, close, period):
        """Calculate Average Directional Index"""
        plus_dm = high.diff()
        minus_dm = low.diff()
        plus_dm[plus_dm < 0] = 0
        minus_dm[minus_dm > 0] = 0
        minus_dm = abs(minus_dm)
        
        tr = self._calculate_atr(high, low, close, period)
        plus_di = 100 * (plus_dm.rolling(window=period).mean() / tr)
        minus_di = 100 * (minus_dm.rolling(window=period).mean() / tr)
        
        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
        return dx.rolling(window=period).mean()
    
    def _calculate_mfi(self, high, low, close, volume, period):
        """Calculate Money Flow Index"""
        typical_price = (high + low + close) / 3
        money_flow = typical_price * volume
        
        positive_flow = money_flow.where(typical_price > typical_price.shift(), 0).rolling(window=period).sum()
        negative_flow = money_flow.where(typical_price < typical_price.shift(), 0).rolling(window=period).sum()
        
        mfi = 100 - (100 / (1 + positive_flow / negative_flow))
        return mfi
    
    def _calculate_obv(self, close, volume):
        """Calculate On Balance Volume"""
        obv = np.where(close > close.shift(), volume, 
                      np.where(close < close.shift(), -volume, 0)).cumsum()
        return pd.Series(obv, index=close.index)
    
    def _calculate_adl(self, high, low, close, volume):
        """Calculate Accumulation/Distribution Line"""
        clv = ((close - low) - (high - close)) / (high - low)
        clv = clv.fillna(0)
        return (clv * volume).cumsum()
    
    def _calculate_cmf(self, high, low, close, volume, period):
        """Calculate Chaikin Money Flow"""
        clv = ((close - low) - (high - close)) / (high - low)
        clv = clv.fillna(0)
        return (clv * volume).rolling(window=period).sum() / volume.rolling(window=period).sum()
    
    def _detect_doji(self, open_price, high, low, close):
        """Detect Doji pattern"""
        body_size = abs(close - open_price)
        total_range = high - low
        return ((body_size <= total_range * 0.1) & (total_range > 0)).astype(int)
    
    def _detect_hammer(self, open_price, high, low, close):
        """Detect Hammer pattern"""
        body_size = abs(close - open_price)
        lower_shadow = np.minimum(open_price, close) - low
        upper_shadow = high - np.maximum(open_price, close)
        return ((lower_shadow > 2 * body_size) & (upper_shadow < body_size)).astype(int)
    
    def _detect_shooting_star(self, open_price, high, low, close):
        """Detect Shooting Star pattern"""
        body_size = abs(close - open_price)
        lower_shadow = np.minimum(open_price, close) - low
        upper_shadow = high - np.maximum(open_price, close)
        return ((upper_shadow > 2 * body_size) & (lower_shadow < body_size)).astype(int)
    
    def _detect_engulfing(self, open_price, high, low, close):
        """Detect Engulfing pattern"""
        prev_body = abs(close.shift() - open_price.shift())
        curr_body = abs(close - open_price)
        return ((curr_body > prev_body) & (close > open_price) & (close.shift() < open_price.shift())).astype(int)
    
    def _calculate_trend_strength(self, prices, period):
        """Calculate trend strength"""
        sma = prices.rolling(window=period).mean()
        return abs(prices - sma) / sma
    
    def _calculate_trend_direction(self, prices, period):
        """Calculate trend direction"""
        sma = prices.rolling(window=period).mean()
        return np.where(prices > sma, 1, -1)
    
    def _calculate_aroon(self, high, low, period=14):
        """Calculate Aroon Oscillator - Zero API cost"""
        try:
            aroon_up = ((period - high.rolling(period).apply(lambda x: period - 1 - x.argmax())) / period) * 100
            aroon_down = ((period - low.rolling(period).apply(lambda x: period - 1 - x.argmin())) / period) * 100
            return {
                'up': aroon_up.fillna(50),
                'down': aroon_down.fillna(50)
            }
        except Exception:
            return {'up': pd.Series(50, index=high.index), 'down': pd.Series(50, index=high.index)}
    
    def _calculate_cmo(self, prices, period=14):
        """Calculate Chande Momentum Oscillator - Zero API cost"""
        try:
            delta = prices.diff()
            up_sum = delta.where(delta > 0, 0).rolling(period).sum()
            down_sum = (-delta.where(delta < 0, 0)).rolling(period).sum()
            cmo = 100 * (up_sum - down_sum) / (up_sum + down_sum)
            return cmo.fillna(0)
        except Exception:
            return pd.Series(0, index=prices.index)
    
    def _detect_head_shoulders(self, high, low, close, lookback=20):
        """Detect Head and Shoulders pattern - Zero API cost"""
        try:
            # Simplified head and shoulders detection
            rolling_max = high.rolling(lookback).max()
            rolling_min = low.rolling(lookback).min()
            
            # Look for three peaks pattern
            peaks = (high == rolling_max) & (high.shift(1) < high) & (high.shift(-1) < high)
            
            # Head and shoulders signal when price breaks neckline
            neckline = (rolling_max + rolling_min) / 2
            signal = (close < neckline) & peaks.rolling(lookback).sum() >= 3
            
            return signal.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _detect_double_top(self, high, close, lookback=20):
        """Detect Double Top pattern - Zero API cost"""
        try:
            rolling_max = high.rolling(lookback).max()
            peaks = (high == rolling_max) & (high.shift(1) < high) & (high.shift(-1) < high)
            
            # Double top when two similar peaks and price breaks support
            recent_peaks = peaks.rolling(lookback*2).sum()
            support_level = close.rolling(lookback).min()
            
            signal = (recent_peaks >= 2) & (close < support_level)
            return signal.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _detect_double_bottom(self, low, close, lookback=20):
        """Detect Double Bottom pattern - Zero API cost"""
        try:
            rolling_min = low.rolling(lookback).min()
            troughs = (low == rolling_min) & (low.shift(1) > low) & (low.shift(-1) > low)
            
            # Double bottom when two similar troughs and price breaks resistance
            recent_troughs = troughs.rolling(lookback*2).sum()
            resistance_level = close.rolling(lookback).max()
            
            signal = (recent_troughs >= 2) & (close > resistance_level)
            return signal.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _detect_triangle_pattern(self, high, low, close, lookback=20):
        """Detect Triangle pattern - Zero API cost"""
        try:
            # Simplified triangle detection based on converging highs and lows
            high_trend = high.rolling(lookback).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
            low_trend = low.rolling(lookback).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
            
            # Triangle when highs are declining and lows are rising (or vice versa)
            converging = (high_trend < 0) & (low_trend > 0) | (high_trend > 0) & (low_trend < 0)
            
            # Breakout signal
            range_size = high.rolling(lookback).max() - low.rolling(lookback).min()
            current_range = high - low
            
            signal = converging & (current_range < range_size * 0.5)
            return signal.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _calculate_price_strength(self, prices, period=14):
        """Calculate Price Strength - Zero API cost"""
        try:
            # Relative strength based on price momentum
            momentum = prices.pct_change(period)
            strength = momentum.rolling(period).rank(pct=True) * 100
            return strength.fillna(50)
        except Exception:
            return pd.Series(50, index=prices.index)
    
    def _calculate_trend_quality(self, prices, volume, period=20):
        """Calculate Trend Quality - Zero API cost"""
        try:
            # Trend quality based on price consistency and volume confirmation
            price_trend = prices.rolling(period).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
            volume_trend = volume.rolling(period).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
            
            # Quality is higher when price and volume trends align
            trend_alignment = np.sign(price_trend) == np.sign(volume_trend)
            price_consistency = 1 - (prices.rolling(period).std() / prices.rolling(period).mean())
            
            quality = (trend_alignment.astype(int) * 50) + (price_consistency * 50)
            return quality.fillna(50).clip(0, 100)
        except Exception:
            return pd.Series(50, index=prices.index)
    
    def _calculate_peg_estimate(self, prices, period=252):
        """Estimate PEG ratio from price momentum - Zero API cost"""
        try:
            # Estimate growth from price momentum over 1 year
            annual_return = prices.pct_change(min(period, len(prices)-1))
            growth_estimate = annual_return * 100  # Convert to percentage
            
            # Estimate P/E from price volatility (lower vol = higher P/E typically)
            volatility = prices.rolling(60).std() / prices.rolling(60).mean()
            pe_estimate = 20 / (volatility * 100 + 0.01)  # Inverse relationship
            
            # PEG = P/E / Growth
            peg_estimate = pe_estimate / (growth_estimate.abs() + 0.01)
            return peg_estimate.fillna(1.5).clip(0, 5)  # Cap at reasonable range
        except Exception:
            return pd.Series(1.5, index=prices.index)
    
    def _calculate_ev_ebitda_proxy(self, prices, volume, period=60):
        """Estimate EV/EBITDA proxy from price and volume - Zero API cost"""
        try:
            # Use price-to-volume ratio as proxy for valuation
            avg_price = prices.rolling(period).mean()
            avg_volume = volume.rolling(period).mean()
            
            # Higher price relative to volume suggests higher valuation
            pv_ratio = avg_price / (avg_volume / 1000000 + 0.01)  # Normalize volume
            
            # Scale to typical EV/EBITDA range (5-25)
            ev_ebitda_proxy = (pv_ratio / pv_ratio.rolling(period*2).mean()).fillna(1) * 12
            return ev_ebitda_proxy.clip(3, 50)
        except Exception:
            return pd.Series(12, index=prices.index)
    
    def _calculate_liquidity_score(self, prices, volume, period=30):
        """Calculate liquidity score (Current Ratio proxy) - Zero API cost"""
        try:
            # Use volume consistency as liquidity proxy
            volume_cv = volume.rolling(period).std() / volume.rolling(period).mean()
            price_stability = 1 - (prices.rolling(period).std() / prices.rolling(period).mean())
            
            # Higher volume consistency + price stability = better liquidity
            liquidity_score = (1 - volume_cv.fillna(0.5)) * 50 + price_stability.fillna(0.5) * 50
            return liquidity_score.clip(0, 100)
        except Exception:
            return pd.Series(50, index=prices.index)
    
    def _estimate_dividend_yield(self, prices, period=252):
        """Estimate dividend yield from price behavior - Zero API cost"""
        try:
            # Mature, stable stocks (lower volatility) typically have higher dividends
            volatility = prices.rolling(period).std() / prices.rolling(period).mean()
            
            # Inverse relationship: lower volatility = higher dividend yield estimate
            dividend_estimate = (0.1 - volatility.fillna(0.05)).clip(0, 0.08) * 100
            return dividend_estimate
        except Exception:
            return pd.Series(2.0, index=prices.index)
    
    def _calculate_fcf_proxy(self, prices, volume, period=90):
        """Calculate Free Cash Flow proxy - Zero API cost"""
        try:
            # Use price momentum and volume as FCF proxy
            price_momentum = prices.pct_change(period)
            volume_trend = volume.rolling(period).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
            
            # Positive price momentum + increasing volume = good FCF proxy
            fcf_proxy = (price_momentum.fillna(0) * 50) + (volume_trend.fillna(0) * 0.001)
            return fcf_proxy.fillna(0).clip(-50, 50)
        except Exception:
            return pd.Series(0, index=prices.index)
    
    def _detect_doji(self, open_price, high, low, close):
        """Detect Doji candlestick pattern - Zero API cost"""
        try:
            body_size = abs(close - open_price)
            total_range = high - low
            
            # Doji when body is very small relative to total range
            doji_signal = (body_size <= total_range * 0.1) & (total_range > 0)
            return doji_signal.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _detect_engulfing(self, open_price, high, low, close):
        """Detect Engulfing pattern - Zero API cost"""
        try:
            # Current candle body
            curr_body = abs(close - open_price)
            curr_bullish = close > open_price
            
            # Previous candle body
            prev_body = abs(close.shift(1) - open_price.shift(1))
            prev_bullish = close.shift(1) > open_price.shift(1)
            
            # Bullish engulfing: current bullish candle engulfs previous bearish
            bullish_engulfing = (curr_bullish & ~prev_bullish & 
                               (curr_body > prev_body * 1.2))
            
            # Bearish engulfing: current bearish candle engulfs previous bullish
            bearish_engulfing = (~curr_bullish & prev_bullish & 
                               (curr_body > prev_body * 1.2))
            
            return (bullish_engulfing | bearish_engulfing).astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _detect_morning_star(self, open_price, high, low, close):
        """Detect Morning Star pattern - Zero API cost"""
        try:
            # Three-candle pattern: bearish, small body, bullish
            bearish_1 = close.shift(2) < open_price.shift(2)
            small_body_2 = abs(close.shift(1) - open_price.shift(1)) < abs(close.shift(2) - open_price.shift(2)) * 0.3
            bullish_3 = close > open_price
            
            # Gap down and gap up
            gap_down = low.shift(1) < close.shift(2)
            gap_up = open_price < close.shift(1)
            
            morning_star = bearish_1 & small_body_2 & bullish_3 & gap_down & gap_up
            return morning_star.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _detect_golden_cross(self, sma_50, sma_200):
        """Detect Golden Cross (50-day SMA crosses above 200-day SMA) - Zero API cost"""
        try:
            # Golden cross when 50-day crosses above 200-day
            cross_above = (sma_50 > sma_200) & (sma_50.shift(1) <= sma_200.shift(1))
            return cross_above.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=sma_50.index)
    
    def _detect_death_cross(self, sma_50, sma_200):
        """Detect Death Cross (50-day SMA crosses below 200-day SMA) - Zero API cost"""
        try:
            # Death cross when 50-day crosses below 200-day
            cross_below = (sma_50 < sma_200) & (sma_50.shift(1) >= sma_200.shift(1))
            return cross_below.astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=sma_50.index)
    
    def _detect_breakout(self, high, low, close, volume, period=20):
        """Detect breakout with volume confirmation - Zero API cost"""
        try:
            # Resistance and support levels
            resistance = high.rolling(period).max().shift(1)
            support = low.rolling(period).min().shift(1)
            
            # Volume confirmation
            avg_volume = volume.rolling(period).mean()
            volume_surge = volume > avg_volume * 1.5
            
            # Breakout signals
            bullish_breakout = (close > resistance) & volume_surge
            bearish_breakdown = (close < support) & volume_surge
            
            return (bullish_breakout | bearish_breakdown).astype(int).fillna(0)
        except Exception:
            return pd.Series(0, index=close.index)
    
    def _get_yahoo_news(self, symbol):
        """Get news from Yahoo Finance"""
        try:
            url = f"https://finance.yahoo.com/quote/{symbol}/news"
            response = self.session.get(url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_items = []
            for item in soup.find_all('h3', class_='Mb(5px)'):
                title = item.get_text().strip()
                if title:
                    news_items.append({'title': title, 'source': 'Yahoo Finance'})
            
            return news_items[:20]
        except Exception as e:
            print(f"Error getting Yahoo news for {symbol}: {e}")
            return []
    
    def _get_google_news(self, symbol):
        """Get news from Google News"""
        try:
            url = f"https://news.google.com/search?q={symbol}+stock&hl=en&gl=US&ceid=US:en"
            response = self.session.get(url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_items = []
            for item in soup.find_all('h3', class_='ipQwMb'):
                title = item.get_text().strip()
                if title:
                    news_items.append({'title': title, 'source': 'Google News'})
            
            return news_items[:20]
        except Exception as e:
            print(f"Error getting Google news for {symbol}: {e}")
            return []
    
    def _get_reddit_sentiment(self, symbol):
        """Get Reddit sentiment (simplified)"""
        try:
            return {'sentiment': 50, 'mentions': 0, 'subreddits': []}
        except Exception as e:
            return {'sentiment': 50, 'mentions': 0, 'subreddits': []}
    
    def _get_twitter_sentiment(self, symbol):
        """Get Twitter sentiment (simplified)"""
        try:
            return {'sentiment': 50, 'mentions': 0, 'hashtags': []}
        except Exception as e:
            return {'sentiment': 50, 'mentions': 0, 'hashtags': []}
    
    def _get_insider_trading(self, symbol):
        """Get insider trading data from free sources"""
        try:
            return {
                'insider_buys': 0,
                'insider_sells': 0,
                'net_insider_activity': 0,
                'insider_confidence': 50
            }
        except Exception as e:
            return {'insider_buys': 0, 'insider_sells': 0, 'net_insider_activity': 0, 'insider_confidence': 50}
    
    def _get_options_data(self, symbol):
        """Get options data"""
        try:
            if getattr(self, 'data_mode', None) == "light":
                return {'put_call_ratio': 1.0, 'implied_volatility': 0.2, 'options_volume': 0}
            ticker = yf.Ticker(symbol)
            options = ticker.option_chain()
            
            if options.calls.empty and options.puts.empty:
                return {'put_call_ratio': 1.0, 'implied_volatility': 0.2, 'options_volume': 0}
            
            put_volume = options.puts['volume'].sum() if not options.puts.empty else 0
            call_volume = options.calls['volume'].sum() if not options.calls.empty else 0
            put_call_ratio = put_volume / call_volume if call_volume > 0 else 1.0
            
            iv = 0
            if not options.calls.empty:
                iv = options.calls['impliedVolatility'].mean()
            elif not options.puts.empty:
                iv = options.puts['impliedVolatility'].mean()
            
            return {
                'put_call_ratio': put_call_ratio,
                'implied_volatility': iv if not pd.isna(iv) else 0.2,
                'options_volume': put_volume + call_volume
            }
        except Exception as e:
            print(f"Error getting options data for {symbol}: {e}")
            return {'put_call_ratio': 1.0, 'implied_volatility': 0.2, 'options_volume': 0}
    
    def _get_institutional_holdings(self, symbol):
        """Get institutional holdings data"""
        try:
            return {
                'institutional_ownership': 0.5,
                'institutional_confidence': 50,
                'hedge_fund_activity': 0
            }
        except Exception as e:
            return {'institutional_ownership': 0.5, 'institutional_confidence': 50, 'hedge_fund_activity': 0}
    
    def _get_earnings_data(self, symbol):
        """Get comprehensive earnings data like professional analysts"""
        try:
            if getattr(self, 'data_mode', None) == "light":
                return {
                    'next_earnings_date': None,
                    'earnings_growth': None,
                    'revenue_growth': None,
                    'profit_margins': None,
                    'return_on_equity': None,
                    'earnings_surprise': None,
                    'earnings_beat_rate': None,
                    'earnings_quality_score': None,
                    'forward_pe': None,
                    'peg_ratio': None,
                    'earnings_consensus': None,
                    'revenue_consensus': None,
                }
            ticker = yf.Ticker(symbol)
            info = ticker.info
            
            # Get earnings history and estimates
            earnings_history = ticker.earnings_history
            earnings_dates = ticker.earnings_dates
            
            # Calculate earnings surprise trends
            earnings_surprise = 0
            earnings_beat_rate = 0
            if not earnings_history.empty:
                recent_earnings = earnings_history.tail(4)  # Last 4 quarters
                if 'Surprise' in recent_earnings.columns:
                    earnings_surprise = recent_earnings['Surprise'].mean()
                    earnings_beat_rate = (recent_earnings['Surprise'] > 0).mean() * 100
            
            # Get forward guidance
            forward_pe = info.get('forwardPE', 0)
            peg_ratio = info.get('pegRatio', 0)
            
            # Calculate earnings quality metrics
            earnings_quality_score = 50  # Base score
            if earnings_beat_rate > 75:
                earnings_quality_score += 25
            elif earnings_beat_rate > 50:
                earnings_quality_score += 10
            elif earnings_beat_rate < 25:
                earnings_quality_score -= 25
            
            if earnings_surprise > 0.05:  # 5% positive surprise
                earnings_quality_score += 15
            elif earnings_surprise < -0.05:  # 5% negative surprise
                earnings_quality_score -= 15
            
            return {
                'next_earnings_date': info.get('earningsDate', None),
                'earnings_growth': info.get('earningsGrowth', 0),
                'revenue_growth': info.get('revenueGrowth', 0),
                'profit_margins': info.get('profitMargins', 0),
                'return_on_equity': info.get('returnOnEquity', 0),
                'earnings_surprise': earnings_surprise,
                'earnings_beat_rate': earnings_beat_rate,
                'earnings_quality_score': max(0, min(100, earnings_quality_score)),
                'forward_pe': forward_pe,
                'peg_ratio': peg_ratio,
                'earnings_consensus': info.get('earningsQuarterlyGrowth', 0),
                'revenue_consensus': info.get('revenueQuarterlyGrowth', 0)
            }
        except Exception as e:
            return {
                'next_earnings_date': None, 'earnings_growth': 0, 'revenue_growth': 0, 
                'profit_margins': 0, 'return_on_equity': 0, 'earnings_surprise': 0,
                'earnings_beat_rate': 50, 'earnings_quality_score': 50, 'forward_pe': 0,
                'peg_ratio': 0, 'earnings_consensus': 0, 'revenue_consensus': 0
            }
    
    def _get_economic_indicators(self):
        """Get economic indicators"""
        try:
            return {
                'vix': None,
                'fed_rate': None,
                'gdp_growth': None,
                'inflation': None,
                'unemployment': None
            }
        except Exception as e:
            return {'vix': None, 'fed_rate': None, 'gdp_growth': None, 'inflation': None, 'unemployment': None}
    
    def _get_analyst_ratings(self, symbol):
        """Get comprehensive analyst ratings like professional traders track"""
        try:
            if getattr(self, 'data_mode', None) == "light":
                return {
                    'analyst_rating': 'Hold',
                    'price_target': 0.0,
                    'rating_changes': 0,
                    'analyst_consensus': 0.0,
                    'analyst_confidence': 50,
                }
            ticker = yf.Ticker(symbol)
            info = ticker.info
            
            # Get analyst recommendations
            recommendations = ticker.recommendations
            
            # Calculate analyst consensus
            analyst_rating = 'Hold'
            price_target = 0
            rating_changes = 0
            analyst_consensus = 0
            
            if not recommendations.empty:
                recent_recs = recommendations.tail(10)  # Last 10 recommendations
                if 'To Grade' in recent_recs.columns:
                    # Count recent rating changes
                    rating_changes = len(recent_recs[recent_recs['To Grade'] != recent_recs['To Grade'].shift()])
                    
                    # Calculate consensus
                    latest_ratings = recent_recs['To Grade'].value_counts()
                    if not latest_ratings.empty:
                        if 'Buy' in latest_ratings.index and latest_ratings['Buy'] > latest_ratings.get('Hold', 0):
                            analyst_rating = 'Buy'
                        elif 'Sell' in latest_ratings.index and latest_ratings['Sell'] > latest_ratings.get('Hold', 0):
                            analyst_rating = 'Sell'
            
            # Get price targets from info
            target_high = info.get('targetHighPrice', 0)
            target_low = info.get('targetLowPrice', 0)
            target_mean = info.get('targetMeanPrice', 0)
            
            if target_mean > 0:
                price_target = target_mean
            elif target_high > 0 and target_low > 0:
                price_target = (target_high + target_low) / 2
            
            # Calculate consensus upside/downside
            current_price = info.get('currentPrice', 0)
            if current_price > 0 and price_target > 0:
                analyst_consensus = (price_target - current_price) / current_price
            
            # Calculate analyst confidence score
            analyst_confidence = 50  # Base score
            if analyst_rating == 'Buy' and analyst_consensus > 0.1:
                analyst_confidence += 30
            elif analyst_rating == 'Buy' and analyst_consensus > 0.05:
                analyst_confidence += 20
            elif analyst_rating == 'Sell' and analyst_consensus < -0.1:
                analyst_confidence -= 30
            elif analyst_rating == 'Sell' and analyst_consensus < -0.05:
                analyst_confidence -= 20
            
            # Factor in rating changes
            if rating_changes > 0:
                recent_changes = recommendations.tail(rating_changes)
                upgrades = len(recent_changes[recent_changes['To Grade'].isin(['Buy', 'Strong Buy'])])
                downgrades = len(recent_changes[recent_changes['To Grade'].isin(['Sell', 'Strong Sell'])])
                
                if upgrades > downgrades:
                    analyst_confidence += 15
                elif downgrades > upgrades:
                    analyst_confidence -= 15
            
            return {
                'analyst_rating': analyst_rating,
                'price_target': price_target,
                'rating_changes': rating_changes,
                'analyst_consensus': analyst_consensus,
                'analyst_confidence': max(0, min(100, analyst_confidence)),
                'target_high': target_high,
                'target_low': target_low,
                'target_mean': target_mean,
                'num_analysts': info.get('numberOfAnalystOpinions', 0)
            }
        except Exception as e:
            return {
                'analyst_rating': 'Hold', 'price_target': 0, 'rating_changes': 0, 
                'analyst_consensus': 0, 'analyst_confidence': 50, 'target_high': 0,
                'target_low': 0, 'target_mean': 0, 'num_analysts': 0
            }
